{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3BVBkjHUeRr"
   },
   "source": [
    "## Mini-Project 3:  Computer Vision using GPU and Transfer Learning\n",
    "#### CSC 180 Intelligent Systems<br>Dr. Haiquan Chen, California State University, Sacramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDrZtNFqMOQa"
   },
   "source": [
    "## Jabari Crenshaw, 219953891\n",
    "## John Kieren, 301144467\n",
    "### CSC 180, Project 3<br>Due: 28 October 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzK7R_YTMx2k"
   },
   "source": [
    "## Helpful Functions for Tensorflow (Little Gems)\n",
    "\n",
    "The following functions will be used with TensorFlow to help preprocess the data.  They allow you to build the feature vector for a neural network. \n",
    "\n",
    "* Predictors/Inputs \n",
    "    * Fill any missing inputs with the median for that column.  Use **missing_median**.\n",
    "    * Encode textual/categorical values with **encode_text_dummy**.\n",
    "    * Encode numeric values with **encode_numeric_zscore**.\n",
    "* Output\n",
    "    * Discard rows with missing outputs.\n",
    "    * Encode textual/categorical values with **encode_text_index**.\n",
    "    * Do not encode output numeric values.\n",
    "* Produce final feature vectors (x) and expected output (y) with **to_xy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "StfmaP0cU6YZ"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6lDBk7KNAIb"
   },
   "source": [
    "## Switch and Verify GPU\n",
    "\n",
    "### To enable GPU backend for your notebook. Runtime->Change runtime type->Hardware Accelerator->GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "amVxcIpqNCFq",
    "outputId": "e5dbdea5-f6d3-4e11-e23b-f7d653cb7754"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt7UKV1lNKCM"
   },
   "source": [
    "### If the above code output '/device:GPU:0',  you have switched to GPU successfully and you are ready to go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdEPP_1dNL0a"
   },
   "source": [
    "## Part I:   Image classification without transfer learning\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIjnlgXXNQql",
    "outputId": "b0968e44-dc89-4b65-dcf8-592fa3c62533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#  Load cifar-10 data and split it to training and test\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FamxhqDFNh8g",
    "outputId": "29e3d075-beae-4759-cda9-9f0cc47afcc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# print out data shape\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "Goy60qXkNlu1",
    "outputId": "fb85c673-041d-46eb-85e6-a70970cf757d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image (#19321): Which is picture of [7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\ny_train[idx] == Label\\ninfer corresponding Description\\n\\nLabel \\tDescription\\n0 \\tairplane\\n1 \\tautomobile\\n2 \\tbird\\n3 \\tcat\\n4 \\tdeer\\n5 \\tdog\\n6 \\tfrog\\n7 \\thorse\\n8 \\tship\\n9 \\ttruck\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print out a random image in x_train as numpy array ...\n",
    "  # ... and print it out as image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = np.random.randint(1,x_train.shape[0])\n",
    "img = x_train[idx]\n",
    "plt.imshow(img)\n",
    "\n",
    "print(f'Image (#{idx}): Which is picture of {y_train[idx]}')\n",
    "\n",
    "'''\n",
    "y_train[idx] == Label\n",
    "infer corresponding Description\n",
    "\n",
    "Label \tDescription\n",
    "0 \tairplane\n",
    "1 \tautomobile\n",
    "2 \tbird\n",
    "3 \tcat\n",
    "4 \tdeer\n",
    "5 \tdog\n",
    "6 \tfrog\n",
    "7 \thorse\n",
    "8 \tship\n",
    "9 \ttruck\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBMSauA5OAHw",
    "outputId": "d59b9339-7d0f-4571-84f6-72f7b9924b02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (50000,)\n",
      "y_test shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Convert y_train from 2D to 1D \n",
    "y_train = y_train.reshape(50000)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# Convert y_test from 2D to 1D \n",
    "y_test = y_test.reshape(10000)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3Wvl8bhiOPCE"
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to one hot format\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOZOPLvcOcVj",
    "outputId": "874d790a-634e-4a00-fa53-ad263baa1a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) \n",
      " (10000, 32, 32, 3) \n",
      " (50000, 10) \n",
      " (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Convert data from int to float and normalize it\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print(x_train.shape,'\\n',\n",
    "      x_test.shape,'\\n',\n",
    "      y_train.shape,'\\n',\n",
    "      y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO-5_6-yOhKo"
   },
   "source": [
    "###  Write your code in the cell below to create a CNN model which contains the following types of operations (layers):   \n",
    "\n",
    "- Conv2D\n",
    "- Activation\n",
    "- MaxPooling2D\n",
    "- Flatten\n",
    "- Dropout\n",
    "- Dense\n",
    "\n",
    "### (optional) You are also encouraged to create multiple models with different activiation functions, different numbers of neurons and layers for performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "qv2AziGhOmAz",
    "outputId": "1ddfe4f0-5cb6-4eb7-e687-84695a95d56c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n#Conv2D\\nmodel.add( Conv2D(32, kernel_size=(3,3), strides=(1, 1), padding='valid', activation='relu', input_shape=(32,32,3) ) )\\n\\n#Conv2d\\nmodel.add( Conv2D(64, kernel_size=(3,3),activation='relu', padding='same') )\\n\\n#MaxPooling2D\\nmodel.add(MaxPooling2D( pool_size=(2,2), strides=None ))\\n\\n#Dropout\\nmodel.add( Dropout( 0.33 ) )\\n\\n#Flatten\\nmodel.add( Flatten() )\\n\\n#Dense\\nmodel.add( Dense(128, activation='relu') )\\n\\n#Dense\\nmodel.add( Dense(num_classes, activation='relu') )\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu', padding='same',\n",
    "                 input_shape=(32,32,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "'''\n",
    "#Conv2D\n",
    "model.add( Conv2D(32, kernel_size=(3,3), strides=(1, 1), padding='valid', activation='relu', input_shape=(32,32,3) ) )\n",
    "\n",
    "#Conv2d\n",
    "model.add( Conv2D(64, kernel_size=(3,3),activation='relu', padding='same') )\n",
    "\n",
    "#MaxPooling2D\n",
    "model.add(MaxPooling2D( pool_size=(2,2), strides=None ))\n",
    "\n",
    "#Dropout\n",
    "model.add( Dropout( 0.33 ) )\n",
    "\n",
    "#Flatten\n",
    "model.add( Flatten() )\n",
    "\n",
    "#Dense\n",
    "model.add( Dense(128, activation='relu') )\n",
    "\n",
    "#Dense\n",
    "model.add( Dense(num_classes, activation='relu') )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kibPifIJOpKe",
    "outputId": "09969a59-65d6-4295-e00c-9baa5ed1c638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              4097000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,160,706\n",
      "Trainable params: 4,160,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQbwniTDOsW4"
   },
   "source": [
    "### Write your code in the cell below for compile, earlystopping and fit. Notice that you should use earlystopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le01kMukvES6"
   },
   "source": [
    "Test Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzGADqb5Oubm",
    "outputId": "e16e9b75-c16e-414e-cc91-4d6caec33ac5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "79/79 - 12s - loss: 1.8367 - accuracy: 0.3288 - val_loss: 1.5365 - val_accuracy: 0.4497 - 12s/epoch - 157ms/step\n",
      "Epoch 2/256\n",
      "79/79 - 1s - loss: 1.4298 - accuracy: 0.4890 - val_loss: 1.4095 - val_accuracy: 0.4957 - 733ms/epoch - 9ms/step\n",
      "Epoch 3/256\n",
      "79/79 - 1s - loss: 1.2420 - accuracy: 0.5593 - val_loss: 1.4234 - val_accuracy: 0.5063 - 655ms/epoch - 8ms/step\n",
      "Epoch 4/256\n",
      "79/79 - 1s - loss: 1.0960 - accuracy: 0.6095 - val_loss: 1.2137 - val_accuracy: 0.5757 - 662ms/epoch - 8ms/step\n",
      "Epoch 5/256\n",
      "79/79 - 1s - loss: 0.9680 - accuracy: 0.6583 - val_loss: 1.2265 - val_accuracy: 0.5783 - 653ms/epoch - 8ms/step\n",
      "Epoch 6/256\n",
      "79/79 - 1s - loss: 0.8098 - accuracy: 0.7159 - val_loss: 1.2953 - val_accuracy: 0.5700 - 666ms/epoch - 8ms/step\n",
      "Epoch 7/256\n",
      "79/79 - 1s - loss: 0.6794 - accuracy: 0.7625 - val_loss: 1.3087 - val_accuracy: 0.5740 - 642ms/epoch - 8ms/step\n",
      "Epoch 8/256\n",
      "79/79 - 1s - loss: 0.5621 - accuracy: 0.8047 - val_loss: 1.2368 - val_accuracy: 0.5990 - 655ms/epoch - 8ms/step\n",
      "Epoch 9/256\n",
      "79/79 - 1s - loss: 0.4228 - accuracy: 0.8563 - val_loss: 1.3216 - val_accuracy: 0.6143 - 645ms/epoch - 8ms/step\n",
      "Epoch 10/256\n",
      "79/79 - 1s - loss: 0.2825 - accuracy: 0.9116 - val_loss: 1.5562 - val_accuracy: 0.5817 - 712ms/epoch - 9ms/step\n",
      "Epoch 11/256\n",
      "79/79 - 1s - loss: 0.2252 - accuracy: 0.9327 - val_loss: 1.5869 - val_accuracy: 0.6060 - 714ms/epoch - 9ms/step\n",
      "Epoch 12/256\n",
      "79/79 - 1s - loss: 0.1463 - accuracy: 0.9587 - val_loss: 1.6668 - val_accuracy: 0.5990 - 712ms/epoch - 9ms/step\n",
      "Epoch 13/256\n",
      "79/79 - 1s - loss: 0.0849 - accuracy: 0.9800 - val_loss: 1.7754 - val_accuracy: 0.6117 - 705ms/epoch - 9ms/step\n",
      "Epoch 14/256\n",
      "79/79 - 1s - loss: 0.0403 - accuracy: 0.9937 - val_loss: 1.9671 - val_accuracy: 0.6147 - 708ms/epoch - 9ms/step\n",
      "Epoch 15/256\n",
      "79/79 - 1s - loss: 0.0229 - accuracy: 0.9977 - val_loss: 2.0427 - val_accuracy: 0.6193 - 720ms/epoch - 9ms/step\n",
      "Epoch 16/256\n",
      "79/79 - 1s - loss: 0.0162 - accuracy: 0.9977 - val_loss: 2.1634 - val_accuracy: 0.6137 - 703ms/epoch - 9ms/step\n",
      "Epoch 17/256\n",
      "79/79 - 1s - loss: 0.0095 - accuracy: 0.9997 - val_loss: 2.1959 - val_accuracy: 0.6180 - 709ms/epoch - 9ms/step\n",
      "Epoch 18/256\n",
      "79/79 - 1s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.3157 - val_accuracy: 0.6203 - 707ms/epoch - 9ms/step\n",
      "Epoch 19/256\n",
      "79/79 - 1s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.3773 - val_accuracy: 0.6263 - 723ms/epoch - 9ms/step\n",
      "Epoch 20/256\n",
      "79/79 - 1s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.4441 - val_accuracy: 0.6227 - 693ms/epoch - 9ms/step\n",
      "Epoch 21/256\n",
      "79/79 - 1s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.4848 - val_accuracy: 0.6233 - 1s/epoch - 13ms/step\n",
      "Epoch 22/256\n",
      "79/79 - 1s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.5186 - val_accuracy: 0.6203 - 1s/epoch - 13ms/step\n",
      "Epoch 23/256\n",
      "79/79 - 1s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.5602 - val_accuracy: 0.6217 - 954ms/epoch - 12ms/step\n",
      "Epoch 24/256\n",
      "79/79 - 1s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.5954 - val_accuracy: 0.6207 - 1s/epoch - 14ms/step\n",
      "Epoch 25/256\n",
      "79/79 - 1s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.6270 - val_accuracy: 0.6197 - 991ms/epoch - 13ms/step\n",
      "Epoch 26/256\n",
      "79/79 - 1s - loss: 9.0593e-04 - accuracy: 1.0000 - val_loss: 2.6650 - val_accuracy: 0.6220 - 734ms/epoch - 9ms/step\n",
      "Epoch 27/256\n",
      "79/79 - 1s - loss: 8.1318e-04 - accuracy: 1.0000 - val_loss: 2.6876 - val_accuracy: 0.6210 - 700ms/epoch - 9ms/step\n",
      "Epoch 28/256\n",
      "79/79 - 1s - loss: 7.2716e-04 - accuracy: 1.0000 - val_loss: 2.7126 - val_accuracy: 0.6197 - 707ms/epoch - 9ms/step\n",
      "Epoch 29/256\n",
      "79/79 - 1s - loss: 6.5815e-04 - accuracy: 1.0000 - val_loss: 2.7410 - val_accuracy: 0.6210 - 665ms/epoch - 8ms/step\n",
      "Epoch 30/256\n",
      "79/79 - 1s - loss: 5.9161e-04 - accuracy: 1.0000 - val_loss: 2.7613 - val_accuracy: 0.6220 - 663ms/epoch - 8ms/step\n",
      "Epoch 31/256\n",
      "79/79 - 1s - loss: 5.4047e-04 - accuracy: 1.0000 - val_loss: 2.7886 - val_accuracy: 0.6213 - 711ms/epoch - 9ms/step\n",
      "Epoch 32/256\n",
      "79/79 - 1s - loss: 4.9729e-04 - accuracy: 1.0000 - val_loss: 2.8133 - val_accuracy: 0.6207 - 672ms/epoch - 9ms/step\n",
      "Epoch 33/256\n",
      "79/79 - 1s - loss: 4.6229e-04 - accuracy: 1.0000 - val_loss: 2.8299 - val_accuracy: 0.6193 - 674ms/epoch - 9ms/step\n",
      "Epoch 34/256\n",
      "79/79 - 1s - loss: 4.2541e-04 - accuracy: 1.0000 - val_loss: 2.8483 - val_accuracy: 0.6203 - 704ms/epoch - 9ms/step\n",
      "Epoch 35/256\n",
      "79/79 - 1s - loss: 3.8546e-04 - accuracy: 1.0000 - val_loss: 2.8763 - val_accuracy: 0.6173 - 668ms/epoch - 8ms/step\n",
      "Epoch 36/256\n",
      "79/79 - 1s - loss: 3.5742e-04 - accuracy: 1.0000 - val_loss: 2.8929 - val_accuracy: 0.6187 - 657ms/epoch - 8ms/step\n",
      "Epoch 37/256\n",
      "79/79 - 1s - loss: 3.2985e-04 - accuracy: 1.0000 - val_loss: 2.9112 - val_accuracy: 0.6210 - 706ms/epoch - 9ms/step\n",
      "Epoch 38/256\n",
      "79/79 - 1s - loss: 3.0482e-04 - accuracy: 1.0000 - val_loss: 2.9332 - val_accuracy: 0.6217 - 712ms/epoch - 9ms/step\n",
      "Epoch 39/256\n",
      "79/79 - 1s - loss: 2.8266e-04 - accuracy: 1.0000 - val_loss: 2.9577 - val_accuracy: 0.6197 - 723ms/epoch - 9ms/step\n",
      "Epoch 40/256\n",
      "79/79 - 1s - loss: 2.6218e-04 - accuracy: 1.0000 - val_loss: 2.9660 - val_accuracy: 0.6210 - 661ms/epoch - 8ms/step\n",
      "Epoch 41/256\n",
      "79/79 - 1s - loss: 2.4523e-04 - accuracy: 1.0000 - val_loss: 2.9871 - val_accuracy: 0.6203 - 669ms/epoch - 8ms/step\n",
      "Epoch 42/256\n",
      "79/79 - 1s - loss: 2.2801e-04 - accuracy: 1.0000 - val_loss: 3.0061 - val_accuracy: 0.6230 - 652ms/epoch - 8ms/step\n",
      "Epoch 43/256\n",
      "79/79 - 1s - loss: 2.1300e-04 - accuracy: 1.0000 - val_loss: 3.0265 - val_accuracy: 0.6200 - 706ms/epoch - 9ms/step\n",
      "Epoch 44/256\n",
      "79/79 - 1s - loss: 2.0033e-04 - accuracy: 1.0000 - val_loss: 3.0435 - val_accuracy: 0.6220 - 726ms/epoch - 9ms/step\n",
      "Epoch 45/256\n",
      "79/79 - 1s - loss: 1.8866e-04 - accuracy: 1.0000 - val_loss: 3.0595 - val_accuracy: 0.6210 - 721ms/epoch - 9ms/step\n",
      "Epoch 46/256\n",
      "79/79 - 1s - loss: 1.7747e-04 - accuracy: 1.0000 - val_loss: 3.0796 - val_accuracy: 0.6217 - 711ms/epoch - 9ms/step\n",
      "Epoch 47/256\n",
      "79/79 - 1s - loss: 1.6504e-04 - accuracy: 1.0000 - val_loss: 3.0931 - val_accuracy: 0.6193 - 645ms/epoch - 8ms/step\n",
      "Epoch 48/256\n",
      "79/79 - 1s - loss: 1.5508e-04 - accuracy: 1.0000 - val_loss: 3.1144 - val_accuracy: 0.6200 - 718ms/epoch - 9ms/step\n",
      "Epoch 49/256\n",
      "79/79 - 1s - loss: 1.4535e-04 - accuracy: 1.0000 - val_loss: 3.1263 - val_accuracy: 0.6200 - 660ms/epoch - 8ms/step\n",
      "Epoch 50/256\n",
      "79/79 - 1s - loss: 1.3650e-04 - accuracy: 1.0000 - val_loss: 3.1420 - val_accuracy: 0.6190 - 663ms/epoch - 8ms/step\n",
      "Epoch 51/256\n",
      "79/79 - 1s - loss: 1.2921e-04 - accuracy: 1.0000 - val_loss: 3.1588 - val_accuracy: 0.6187 - 711ms/epoch - 9ms/step\n",
      "Epoch 52/256\n",
      "79/79 - 1s - loss: 1.2241e-04 - accuracy: 1.0000 - val_loss: 3.1707 - val_accuracy: 0.6190 - 669ms/epoch - 8ms/step\n",
      "Epoch 53/256\n",
      "79/79 - 1s - loss: 1.1410e-04 - accuracy: 1.0000 - val_loss: 3.1880 - val_accuracy: 0.6203 - 706ms/epoch - 9ms/step\n",
      "Epoch 54/256\n",
      "79/79 - 1s - loss: 1.0778e-04 - accuracy: 1.0000 - val_loss: 3.2019 - val_accuracy: 0.6213 - 719ms/epoch - 9ms/step\n",
      "Epoch 55/256\n",
      "79/79 - 1s - loss: 1.0152e-04 - accuracy: 1.0000 - val_loss: 3.2229 - val_accuracy: 0.6190 - 655ms/epoch - 8ms/step\n",
      "Epoch 56/256\n",
      "79/79 - 1s - loss: 9.6357e-05 - accuracy: 1.0000 - val_loss: 3.2307 - val_accuracy: 0.6177 - 656ms/epoch - 8ms/step\n",
      "Epoch 57/256\n",
      "79/79 - 1s - loss: 9.1931e-05 - accuracy: 1.0000 - val_loss: 3.2440 - val_accuracy: 0.6177 - 676ms/epoch - 9ms/step\n",
      "Epoch 58/256\n",
      "79/79 - 1s - loss: 8.6096e-05 - accuracy: 1.0000 - val_loss: 3.2607 - val_accuracy: 0.6197 - 720ms/epoch - 9ms/step\n",
      "Epoch 59/256\n",
      "79/79 - 1s - loss: 8.0917e-05 - accuracy: 1.0000 - val_loss: 3.2750 - val_accuracy: 0.6183 - 709ms/epoch - 9ms/step\n",
      "Epoch 60/256\n",
      "79/79 - 1s - loss: 7.6912e-05 - accuracy: 1.0000 - val_loss: 3.2919 - val_accuracy: 0.6183 - 703ms/epoch - 9ms/step\n",
      "Epoch 61/256\n",
      "79/79 - 1s - loss: 7.3026e-05 - accuracy: 1.0000 - val_loss: 3.3059 - val_accuracy: 0.6210 - 671ms/epoch - 8ms/step\n",
      "Epoch 62/256\n",
      "79/79 - 1s - loss: 6.8677e-05 - accuracy: 1.0000 - val_loss: 3.3228 - val_accuracy: 0.6200 - 668ms/epoch - 8ms/step\n",
      "Epoch 63/256\n",
      "79/79 - 1s - loss: 6.5243e-05 - accuracy: 1.0000 - val_loss: 3.3338 - val_accuracy: 0.6190 - 709ms/epoch - 9ms/step\n",
      "Epoch 64/256\n",
      "79/79 - 1s - loss: 6.1597e-05 - accuracy: 1.0000 - val_loss: 3.3510 - val_accuracy: 0.6193 - 662ms/epoch - 8ms/step\n",
      "Epoch 65/256\n",
      "79/79 - 1s - loss: 5.9013e-05 - accuracy: 1.0000 - val_loss: 3.3634 - val_accuracy: 0.6200 - 656ms/epoch - 8ms/step\n",
      "Epoch 66/256\n",
      "79/79 - 1s - loss: 5.6129e-05 - accuracy: 1.0000 - val_loss: 3.3735 - val_accuracy: 0.6200 - 663ms/epoch - 8ms/step\n",
      "Epoch 67/256\n",
      "79/79 - 1s - loss: 5.2399e-05 - accuracy: 1.0000 - val_loss: 3.3984 - val_accuracy: 0.6187 - 710ms/epoch - 9ms/step\n",
      "Epoch 68/256\n",
      "79/79 - 1s - loss: 5.0310e-05 - accuracy: 1.0000 - val_loss: 3.4047 - val_accuracy: 0.6193 - 684ms/epoch - 9ms/step\n",
      "Epoch 69/256\n",
      "79/79 - 1s - loss: 4.7114e-05 - accuracy: 1.0000 - val_loss: 3.4150 - val_accuracy: 0.6183 - 906ms/epoch - 11ms/step\n",
      "Epoch 70/256\n",
      "79/79 - 1s - loss: 4.5056e-05 - accuracy: 1.0000 - val_loss: 3.4311 - val_accuracy: 0.6180 - 705ms/epoch - 9ms/step\n",
      "Epoch 71/256\n",
      "79/79 - 1s - loss: 4.2668e-05 - accuracy: 1.0000 - val_loss: 3.4479 - val_accuracy: 0.6187 - 723ms/epoch - 9ms/step\n",
      "Epoch 72/256\n",
      "79/79 - 1s - loss: 4.0580e-05 - accuracy: 1.0000 - val_loss: 3.4582 - val_accuracy: 0.6190 - 711ms/epoch - 9ms/step\n",
      "Epoch 73/256\n",
      "79/79 - 1s - loss: 3.8640e-05 - accuracy: 1.0000 - val_loss: 3.4685 - val_accuracy: 0.6187 - 673ms/epoch - 9ms/step\n",
      "Epoch 74/256\n",
      "79/79 - 1s - loss: 3.6540e-05 - accuracy: 1.0000 - val_loss: 3.4827 - val_accuracy: 0.6180 - 659ms/epoch - 8ms/step\n",
      "Epoch 75/256\n",
      "79/79 - 1s - loss: 3.4896e-05 - accuracy: 1.0000 - val_loss: 3.5016 - val_accuracy: 0.6170 - 714ms/epoch - 9ms/step\n",
      "Epoch 76/256\n",
      "79/79 - 1s - loss: 3.3069e-05 - accuracy: 1.0000 - val_loss: 3.5133 - val_accuracy: 0.6180 - 659ms/epoch - 8ms/step\n",
      "Epoch 77/256\n",
      "79/79 - 1s - loss: 3.1345e-05 - accuracy: 1.0000 - val_loss: 3.5243 - val_accuracy: 0.6183 - 666ms/epoch - 8ms/step\n",
      "Epoch 78/256\n",
      "79/79 - 1s - loss: 2.9762e-05 - accuracy: 1.0000 - val_loss: 3.5382 - val_accuracy: 0.6173 - 652ms/epoch - 8ms/step\n",
      "Epoch 79/256\n",
      "79/79 - 1s - loss: 2.8233e-05 - accuracy: 1.0000 - val_loss: 3.5500 - val_accuracy: 0.6177 - 648ms/epoch - 8ms/step\n",
      "Epoch 80/256\n",
      "79/79 - 1s - loss: 2.7127e-05 - accuracy: 1.0000 - val_loss: 3.5669 - val_accuracy: 0.6173 - 656ms/epoch - 8ms/step\n",
      "Epoch 81/256\n",
      "79/79 - 1s - loss: 2.5699e-05 - accuracy: 1.0000 - val_loss: 3.5759 - val_accuracy: 0.6170 - 659ms/epoch - 8ms/step\n",
      "Epoch 82/256\n",
      "79/79 - 1s - loss: 2.4439e-05 - accuracy: 1.0000 - val_loss: 3.5882 - val_accuracy: 0.6167 - 719ms/epoch - 9ms/step\n",
      "Epoch 83/256\n",
      "79/79 - 1s - loss: 2.3318e-05 - accuracy: 1.0000 - val_loss: 3.6032 - val_accuracy: 0.6163 - 707ms/epoch - 9ms/step\n",
      "Epoch 84/256\n",
      "79/79 - 1s - loss: 2.2222e-05 - accuracy: 1.0000 - val_loss: 3.6149 - val_accuracy: 0.6170 - 661ms/epoch - 8ms/step\n",
      "Epoch 85/256\n",
      "79/79 - 1s - loss: 2.1076e-05 - accuracy: 1.0000 - val_loss: 3.6306 - val_accuracy: 0.6160 - 675ms/epoch - 9ms/step\n",
      "Epoch 86/256\n",
      "79/79 - 1s - loss: 2.0099e-05 - accuracy: 1.0000 - val_loss: 3.6449 - val_accuracy: 0.6187 - 677ms/epoch - 9ms/step\n",
      "Epoch 87/256\n",
      "79/79 - 1s - loss: 1.9256e-05 - accuracy: 1.0000 - val_loss: 3.6512 - val_accuracy: 0.6177 - 657ms/epoch - 8ms/step\n",
      "Epoch 88/256\n",
      "79/79 - 1s - loss: 1.8226e-05 - accuracy: 1.0000 - val_loss: 3.6664 - val_accuracy: 0.6157 - 712ms/epoch - 9ms/step\n",
      "Epoch 89/256\n",
      "79/79 - 1s - loss: 1.7435e-05 - accuracy: 1.0000 - val_loss: 3.6833 - val_accuracy: 0.6163 - 671ms/epoch - 8ms/step\n",
      "Epoch 90/256\n",
      "79/79 - 1s - loss: 1.6480e-05 - accuracy: 1.0000 - val_loss: 3.6915 - val_accuracy: 0.6163 - 713ms/epoch - 9ms/step\n",
      "Epoch 91/256\n",
      "79/79 - 1s - loss: 1.5731e-05 - accuracy: 1.0000 - val_loss: 3.7037 - val_accuracy: 0.6170 - 717ms/epoch - 9ms/step\n",
      "Epoch 92/256\n",
      "79/79 - 1s - loss: 1.5297e-05 - accuracy: 1.0000 - val_loss: 3.7185 - val_accuracy: 0.6177 - 680ms/epoch - 9ms/step\n",
      "Epoch 93/256\n",
      "79/79 - 1s - loss: 1.4304e-05 - accuracy: 1.0000 - val_loss: 3.7301 - val_accuracy: 0.6173 - 710ms/epoch - 9ms/step\n",
      "Epoch 94/256\n",
      "79/79 - 1s - loss: 1.3678e-05 - accuracy: 1.0000 - val_loss: 3.7418 - val_accuracy: 0.6157 - 949ms/epoch - 12ms/step\n",
      "Epoch 95/256\n",
      "79/79 - 1s - loss: 1.3054e-05 - accuracy: 1.0000 - val_loss: 3.7527 - val_accuracy: 0.6163 - 658ms/epoch - 8ms/step\n",
      "Epoch 96/256\n",
      "79/79 - 1s - loss: 1.2550e-05 - accuracy: 1.0000 - val_loss: 3.7683 - val_accuracy: 0.6150 - 713ms/epoch - 9ms/step\n",
      "Epoch 97/256\n",
      "79/79 - 1s - loss: 1.1773e-05 - accuracy: 1.0000 - val_loss: 3.7823 - val_accuracy: 0.6157 - 711ms/epoch - 9ms/step\n",
      "Epoch 98/256\n",
      "79/79 - 1s - loss: 1.1292e-05 - accuracy: 1.0000 - val_loss: 3.7944 - val_accuracy: 0.6163 - 721ms/epoch - 9ms/step\n",
      "Epoch 99/256\n",
      "79/79 - 1s - loss: 1.0778e-05 - accuracy: 1.0000 - val_loss: 3.8034 - val_accuracy: 0.6173 - 670ms/epoch - 8ms/step\n",
      "Epoch 100/256\n",
      "79/79 - 1s - loss: 1.0196e-05 - accuracy: 1.0000 - val_loss: 3.8166 - val_accuracy: 0.6160 - 673ms/epoch - 9ms/step\n",
      "Epoch 101/256\n",
      "79/79 - 1s - loss: 9.7559e-06 - accuracy: 1.0000 - val_loss: 3.8286 - val_accuracy: 0.6157 - 656ms/epoch - 8ms/step\n",
      "Epoch 102/256\n",
      "79/79 - 1s - loss: 9.3092e-06 - accuracy: 1.0000 - val_loss: 3.8434 - val_accuracy: 0.6163 - 717ms/epoch - 9ms/step\n",
      "Epoch 103/256\n",
      "79/79 - 1s - loss: 8.8712e-06 - accuracy: 1.0000 - val_loss: 3.8506 - val_accuracy: 0.6167 - 659ms/epoch - 8ms/step\n",
      "Epoch 104/256\n",
      "79/79 - 1s - loss: 8.4277e-06 - accuracy: 1.0000 - val_loss: 3.8639 - val_accuracy: 0.6153 - 661ms/epoch - 8ms/step\n",
      "Epoch 105/256\n",
      "79/79 - 1s - loss: 8.0764e-06 - accuracy: 1.0000 - val_loss: 3.8747 - val_accuracy: 0.6157 - 715ms/epoch - 9ms/step\n",
      "Epoch 106/256\n",
      "79/79 - 1s - loss: 7.7971e-06 - accuracy: 1.0000 - val_loss: 3.8908 - val_accuracy: 0.6167 - 665ms/epoch - 8ms/step\n",
      "Epoch 107/256\n",
      "79/79 - 1s - loss: 7.4206e-06 - accuracy: 1.0000 - val_loss: 3.9044 - val_accuracy: 0.6160 - 730ms/epoch - 9ms/step\n",
      "Epoch 108/256\n",
      "79/79 - 1s - loss: 7.0545e-06 - accuracy: 1.0000 - val_loss: 3.9150 - val_accuracy: 0.6147 - 725ms/epoch - 9ms/step\n",
      "Epoch 109/256\n",
      "79/79 - 1s - loss: 6.7000e-06 - accuracy: 1.0000 - val_loss: 3.9284 - val_accuracy: 0.6153 - 669ms/epoch - 8ms/step\n",
      "Epoch 110/256\n",
      "79/79 - 1s - loss: 6.4455e-06 - accuracy: 1.0000 - val_loss: 3.9378 - val_accuracy: 0.6137 - 670ms/epoch - 8ms/step\n",
      "Epoch 111/256\n",
      "79/79 - 1s - loss: 6.1199e-06 - accuracy: 1.0000 - val_loss: 3.9539 - val_accuracy: 0.6153 - 657ms/epoch - 8ms/step\n",
      "Epoch 112/256\n",
      "79/79 - 1s - loss: 5.8283e-06 - accuracy: 1.0000 - val_loss: 3.9654 - val_accuracy: 0.6160 - 662ms/epoch - 8ms/step\n",
      "Epoch 113/256\n",
      "79/79 - 1s - loss: 5.5660e-06 - accuracy: 1.0000 - val_loss: 3.9763 - val_accuracy: 0.6147 - 708ms/epoch - 9ms/step\n",
      "Epoch 114/256\n",
      "79/79 - 1s - loss: 5.3398e-06 - accuracy: 1.0000 - val_loss: 3.9854 - val_accuracy: 0.6150 - 667ms/epoch - 8ms/step\n",
      "Epoch 115/256\n",
      "79/79 - 1s - loss: 5.1023e-06 - accuracy: 1.0000 - val_loss: 4.0035 - val_accuracy: 0.6140 - 672ms/epoch - 9ms/step\n",
      "Epoch 116/256\n",
      "79/79 - 1s - loss: 4.8543e-06 - accuracy: 1.0000 - val_loss: 4.0129 - val_accuracy: 0.6160 - 711ms/epoch - 9ms/step\n",
      "Epoch 117/256\n",
      "79/79 - 1s - loss: 4.6588e-06 - accuracy: 1.0000 - val_loss: 4.0284 - val_accuracy: 0.6133 - 665ms/epoch - 8ms/step\n",
      "Epoch 118/256\n",
      "79/79 - 1s - loss: 4.4681e-06 - accuracy: 1.0000 - val_loss: 4.0379 - val_accuracy: 0.6137 - 664ms/epoch - 8ms/step\n",
      "Epoch 119/256\n",
      "79/79 - 1s - loss: 4.2294e-06 - accuracy: 1.0000 - val_loss: 4.0500 - val_accuracy: 0.6160 - 707ms/epoch - 9ms/step\n",
      "Epoch 120/256\n",
      "79/79 - 1s - loss: 4.0490e-06 - accuracy: 1.0000 - val_loss: 4.0577 - val_accuracy: 0.6157 - 662ms/epoch - 8ms/step\n",
      "Epoch 121/256\n",
      "79/79 - 1s - loss: 3.8456e-06 - accuracy: 1.0000 - val_loss: 4.0759 - val_accuracy: 0.6143 - 713ms/epoch - 9ms/step\n",
      "Epoch 122/256\n",
      "79/79 - 1s - loss: 3.6724e-06 - accuracy: 1.0000 - val_loss: 4.0820 - val_accuracy: 0.6143 - 719ms/epoch - 9ms/step\n",
      "Epoch 123/256\n",
      "79/79 - 1s - loss: 3.5127e-06 - accuracy: 1.0000 - val_loss: 4.0938 - val_accuracy: 0.6133 - 721ms/epoch - 9ms/step\n",
      "Epoch 124/256\n",
      "79/79 - 1s - loss: 3.3646e-06 - accuracy: 1.0000 - val_loss: 4.1043 - val_accuracy: 0.6147 - 716ms/epoch - 9ms/step\n",
      "Epoch 125/256\n",
      "79/79 - 1s - loss: 3.2075e-06 - accuracy: 1.0000 - val_loss: 4.1200 - val_accuracy: 0.6147 - 713ms/epoch - 9ms/step\n",
      "Epoch 126/256\n",
      "79/79 - 1s - loss: 3.0664e-06 - accuracy: 1.0000 - val_loss: 4.1347 - val_accuracy: 0.6153 - 715ms/epoch - 9ms/step\n",
      "Epoch 127/256\n",
      "79/79 - 1s - loss: 2.9331e-06 - accuracy: 1.0000 - val_loss: 4.1434 - val_accuracy: 0.6167 - 703ms/epoch - 9ms/step\n",
      "Epoch 128/256\n",
      "79/79 - 1s - loss: 2.8065e-06 - accuracy: 1.0000 - val_loss: 4.1556 - val_accuracy: 0.6147 - 661ms/epoch - 8ms/step\n",
      "Epoch 129/256\n",
      "79/79 - 1s - loss: 2.6873e-06 - accuracy: 1.0000 - val_loss: 4.1676 - val_accuracy: 0.6137 - 706ms/epoch - 9ms/step\n",
      "Epoch 130/256\n",
      "79/79 - 1s - loss: 2.5550e-06 - accuracy: 1.0000 - val_loss: 4.1812 - val_accuracy: 0.6153 - 671ms/epoch - 8ms/step\n",
      "Epoch 131/256\n",
      "79/79 - 1s - loss: 2.4506e-06 - accuracy: 1.0000 - val_loss: 4.1906 - val_accuracy: 0.6150 - 727ms/epoch - 9ms/step\n",
      "Epoch 132/256\n",
      "79/79 - 1s - loss: 2.3389e-06 - accuracy: 1.0000 - val_loss: 4.2049 - val_accuracy: 0.6140 - 709ms/epoch - 9ms/step\n",
      "Epoch 133/256\n",
      "79/79 - 1s - loss: 2.2580e-06 - accuracy: 1.0000 - val_loss: 4.2169 - val_accuracy: 0.6143 - 669ms/epoch - 8ms/step\n",
      "Epoch 134/256\n",
      "79/79 - 1s - loss: 2.1361e-06 - accuracy: 1.0000 - val_loss: 4.2281 - val_accuracy: 0.6137 - 666ms/epoch - 8ms/step\n",
      "Epoch 135/256\n",
      "79/79 - 1s - loss: 2.0478e-06 - accuracy: 1.0000 - val_loss: 4.2367 - val_accuracy: 0.6147 - 702ms/epoch - 9ms/step\n",
      "Epoch 136/256\n",
      "79/79 - 1s - loss: 1.9626e-06 - accuracy: 1.0000 - val_loss: 4.2482 - val_accuracy: 0.6150 - 676ms/epoch - 9ms/step\n",
      "Epoch 137/256\n",
      "79/79 - 1s - loss: 1.8685e-06 - accuracy: 1.0000 - val_loss: 4.2633 - val_accuracy: 0.6143 - 665ms/epoch - 8ms/step\n",
      "Epoch 138/256\n",
      "79/79 - 1s - loss: 1.7906e-06 - accuracy: 1.0000 - val_loss: 4.2708 - val_accuracy: 0.6153 - 712ms/epoch - 9ms/step\n",
      "Epoch 139/256\n",
      "79/79 - 1s - loss: 1.7050e-06 - accuracy: 1.0000 - val_loss: 4.2866 - val_accuracy: 0.6127 - 713ms/epoch - 9ms/step\n",
      "Epoch 140/256\n",
      "79/79 - 1s - loss: 1.6312e-06 - accuracy: 1.0000 - val_loss: 4.2978 - val_accuracy: 0.6133 - 716ms/epoch - 9ms/step\n",
      "Epoch 141/256\n",
      "79/79 - 1s - loss: 1.5684e-06 - accuracy: 1.0000 - val_loss: 4.3066 - val_accuracy: 0.6140 - 716ms/epoch - 9ms/step\n",
      "Epoch 142/256\n",
      "79/79 - 1s - loss: 1.5034e-06 - accuracy: 1.0000 - val_loss: 4.3226 - val_accuracy: 0.6133 - 871ms/epoch - 11ms/step\n",
      "Epoch 143/256\n",
      "79/79 - 1s - loss: 1.4371e-06 - accuracy: 1.0000 - val_loss: 4.3329 - val_accuracy: 0.6140 - 883ms/epoch - 11ms/step\n",
      "Epoch 144/256\n",
      "79/79 - 1s - loss: 1.3673e-06 - accuracy: 1.0000 - val_loss: 4.3459 - val_accuracy: 0.6143 - 872ms/epoch - 11ms/step\n",
      "Epoch 145/256\n",
      "79/79 - 1s - loss: 1.3076e-06 - accuracy: 1.0000 - val_loss: 4.3530 - val_accuracy: 0.6147 - 715ms/epoch - 9ms/step\n",
      "Epoch 146/256\n",
      "79/79 - 1s - loss: 1.2544e-06 - accuracy: 1.0000 - val_loss: 4.3665 - val_accuracy: 0.6143 - 670ms/epoch - 8ms/step\n",
      "Epoch 147/256\n",
      "79/79 - 1s - loss: 1.2052e-06 - accuracy: 1.0000 - val_loss: 4.3763 - val_accuracy: 0.6143 - 714ms/epoch - 9ms/step\n",
      "Epoch 148/256\n",
      "79/79 - 1s - loss: 1.1506e-06 - accuracy: 1.0000 - val_loss: 4.3917 - val_accuracy: 0.6130 - 666ms/epoch - 8ms/step\n",
      "Epoch 149/256\n",
      "79/79 - 1s - loss: 1.1013e-06 - accuracy: 1.0000 - val_loss: 4.4008 - val_accuracy: 0.6133 - 720ms/epoch - 9ms/step\n",
      "Epoch 150/256\n",
      "79/79 - 1s - loss: 1.0557e-06 - accuracy: 1.0000 - val_loss: 4.4129 - val_accuracy: 0.6140 - 665ms/epoch - 8ms/step\n",
      "Epoch 151/256\n",
      "79/79 - 1s - loss: 1.0014e-06 - accuracy: 1.0000 - val_loss: 4.4249 - val_accuracy: 0.6140 - 682ms/epoch - 9ms/step\n",
      "Epoch 152/256\n",
      "79/79 - 1s - loss: 9.6695e-07 - accuracy: 1.0000 - val_loss: 4.4320 - val_accuracy: 0.6130 - 722ms/epoch - 9ms/step\n",
      "Epoch 153/256\n",
      "79/79 - 1s - loss: 9.2376e-07 - accuracy: 1.0000 - val_loss: 4.4468 - val_accuracy: 0.6133 - 663ms/epoch - 8ms/step\n",
      "Epoch 154/256\n",
      "79/79 - 1s - loss: 8.8365e-07 - accuracy: 1.0000 - val_loss: 4.4597 - val_accuracy: 0.6137 - 669ms/epoch - 8ms/step\n",
      "Epoch 155/256\n",
      "79/79 - 1s - loss: 8.4357e-07 - accuracy: 1.0000 - val_loss: 4.4708 - val_accuracy: 0.6140 - 718ms/epoch - 9ms/step\n",
      "Epoch 156/256\n",
      "79/79 - 1s - loss: 8.0761e-07 - accuracy: 1.0000 - val_loss: 4.4813 - val_accuracy: 0.6153 - 714ms/epoch - 9ms/step\n",
      "Epoch 157/256\n",
      "79/79 - 1s - loss: 7.8243e-07 - accuracy: 1.0000 - val_loss: 4.4920 - val_accuracy: 0.6147 - 663ms/epoch - 8ms/step\n",
      "Epoch 158/256\n",
      "79/79 - 1s - loss: 7.4550e-07 - accuracy: 1.0000 - val_loss: 4.5054 - val_accuracy: 0.6127 - 704ms/epoch - 9ms/step\n",
      "Epoch 159/256\n",
      "79/79 - 1s - loss: 7.1256e-07 - accuracy: 1.0000 - val_loss: 4.5168 - val_accuracy: 0.6133 - 680ms/epoch - 9ms/step\n",
      "Epoch 160/256\n",
      "79/79 - 1s - loss: 6.8071e-07 - accuracy: 1.0000 - val_loss: 4.5284 - val_accuracy: 0.6147 - 773ms/epoch - 10ms/step\n",
      "Epoch 161/256\n",
      "79/79 - 1s - loss: 6.5076e-07 - accuracy: 1.0000 - val_loss: 4.5401 - val_accuracy: 0.6127 - 778ms/epoch - 10ms/step\n",
      "Epoch 162/256\n",
      "79/79 - 1s - loss: 6.2482e-07 - accuracy: 1.0000 - val_loss: 4.5509 - val_accuracy: 0.6133 - 734ms/epoch - 9ms/step\n",
      "Epoch 163/256\n",
      "79/79 - 1s - loss: 5.9899e-07 - accuracy: 1.0000 - val_loss: 4.5629 - val_accuracy: 0.6150 - 713ms/epoch - 9ms/step\n",
      "Epoch 164/256\n",
      "79/79 - 1s - loss: 5.7478e-07 - accuracy: 1.0000 - val_loss: 4.5720 - val_accuracy: 0.6147 - 725ms/epoch - 9ms/step\n",
      "Epoch 165/256\n",
      "79/79 - 1s - loss: 5.5088e-07 - accuracy: 1.0000 - val_loss: 4.5891 - val_accuracy: 0.6137 - 716ms/epoch - 9ms/step\n",
      "Epoch 166/256\n",
      "79/79 - 1s - loss: 5.2601e-07 - accuracy: 1.0000 - val_loss: 4.6048 - val_accuracy: 0.6123 - 681ms/epoch - 9ms/step\n",
      "Epoch 167/256\n",
      "79/79 - 1s - loss: 5.0399e-07 - accuracy: 1.0000 - val_loss: 4.6164 - val_accuracy: 0.6137 - 671ms/epoch - 8ms/step\n",
      "Epoch 168/256\n",
      "79/79 - 1s - loss: 4.8294e-07 - accuracy: 1.0000 - val_loss: 4.6236 - val_accuracy: 0.6123 - 713ms/epoch - 9ms/step\n",
      "Epoch 169/256\n",
      "79/79 - 1s - loss: 4.6328e-07 - accuracy: 1.0000 - val_loss: 4.6291 - val_accuracy: 0.6140 - 713ms/epoch - 9ms/step\n",
      "Epoch 170/256\n",
      "79/79 - 1s - loss: 4.4664e-07 - accuracy: 1.0000 - val_loss: 4.6441 - val_accuracy: 0.6133 - 667ms/epoch - 8ms/step\n",
      "Epoch 171/256\n",
      "79/79 - 1s - loss: 4.2496e-07 - accuracy: 1.0000 - val_loss: 4.6551 - val_accuracy: 0.6137 - 673ms/epoch - 9ms/step\n",
      "Epoch 172/256\n",
      "79/79 - 1s - loss: 4.0859e-07 - accuracy: 1.0000 - val_loss: 4.6721 - val_accuracy: 0.6137 - 708ms/epoch - 9ms/step\n",
      "Epoch 173/256\n",
      "79/79 - 1s - loss: 3.9147e-07 - accuracy: 1.0000 - val_loss: 4.6807 - val_accuracy: 0.6120 - 668ms/epoch - 8ms/step\n",
      "Epoch 174/256\n",
      "79/79 - 1s - loss: 3.7347e-07 - accuracy: 1.0000 - val_loss: 4.6885 - val_accuracy: 0.6127 - 711ms/epoch - 9ms/step\n",
      "Epoch 175/256\n",
      "79/79 - 1s - loss: 3.5889e-07 - accuracy: 1.0000 - val_loss: 4.7022 - val_accuracy: 0.6130 - 669ms/epoch - 8ms/step\n",
      "Epoch 176/256\n",
      "79/79 - 1s - loss: 3.4412e-07 - accuracy: 1.0000 - val_loss: 4.7102 - val_accuracy: 0.6137 - 708ms/epoch - 9ms/step\n",
      "Epoch 177/256\n",
      "79/79 - 1s - loss: 3.3212e-07 - accuracy: 1.0000 - val_loss: 4.7229 - val_accuracy: 0.6137 - 714ms/epoch - 9ms/step\n",
      "Epoch 178/256\n",
      "79/79 - 1s - loss: 3.1625e-07 - accuracy: 1.0000 - val_loss: 4.7401 - val_accuracy: 0.6120 - 669ms/epoch - 8ms/step\n",
      "Epoch 179/256\n",
      "79/79 - 1s - loss: 3.0270e-07 - accuracy: 1.0000 - val_loss: 4.7433 - val_accuracy: 0.6130 - 660ms/epoch - 8ms/step\n",
      "Epoch 180/256\n",
      "79/79 - 1s - loss: 2.9074e-07 - accuracy: 1.0000 - val_loss: 4.7564 - val_accuracy: 0.6130 - 665ms/epoch - 8ms/step\n",
      "Epoch 181/256\n",
      "79/79 - 1s - loss: 2.7754e-07 - accuracy: 1.0000 - val_loss: 4.7673 - val_accuracy: 0.6123 - 723ms/epoch - 9ms/step\n",
      "Epoch 182/256\n",
      "79/79 - 1s - loss: 2.6789e-07 - accuracy: 1.0000 - val_loss: 4.7806 - val_accuracy: 0.6120 - 720ms/epoch - 9ms/step\n",
      "Epoch 183/256\n",
      "79/79 - 1s - loss: 2.5520e-07 - accuracy: 1.0000 - val_loss: 4.7935 - val_accuracy: 0.6120 - 667ms/epoch - 8ms/step\n",
      "Epoch 184/256\n",
      "79/79 - 1s - loss: 2.4730e-07 - accuracy: 1.0000 - val_loss: 4.8015 - val_accuracy: 0.6117 - 713ms/epoch - 9ms/step\n",
      "Epoch 185/256\n",
      "79/79 - 1s - loss: 2.3639e-07 - accuracy: 1.0000 - val_loss: 4.8140 - val_accuracy: 0.6127 - 711ms/epoch - 9ms/step\n",
      "Epoch 186/256\n",
      "79/79 - 1s - loss: 2.2595e-07 - accuracy: 1.0000 - val_loss: 4.8342 - val_accuracy: 0.6113 - 675ms/epoch - 9ms/step\n",
      "Epoch 187/256\n",
      "79/79 - 1s - loss: 2.1747e-07 - accuracy: 1.0000 - val_loss: 4.8389 - val_accuracy: 0.6110 - 673ms/epoch - 9ms/step\n",
      "Epoch 188/256\n",
      "79/79 - 1s - loss: 2.0872e-07 - accuracy: 1.0000 - val_loss: 4.8497 - val_accuracy: 0.6110 - 668ms/epoch - 8ms/step\n",
      "Epoch 189/256\n",
      "79/79 - 1s - loss: 1.9939e-07 - accuracy: 1.0000 - val_loss: 4.8603 - val_accuracy: 0.6130 - 712ms/epoch - 9ms/step\n",
      "Epoch 190/256\n",
      "79/79 - 1s - loss: 1.9155e-07 - accuracy: 1.0000 - val_loss: 4.8690 - val_accuracy: 0.6120 - 667ms/epoch - 8ms/step\n",
      "Epoch 191/256\n",
      "79/79 - 1s - loss: 1.8420e-07 - accuracy: 1.0000 - val_loss: 4.8854 - val_accuracy: 0.6107 - 660ms/epoch - 8ms/step\n",
      "Epoch 192/256\n",
      "79/79 - 1s - loss: 1.7576e-07 - accuracy: 1.0000 - val_loss: 4.8924 - val_accuracy: 0.6113 - 662ms/epoch - 8ms/step\n",
      "Epoch 193/256\n",
      "79/79 - 1s - loss: 1.6869e-07 - accuracy: 1.0000 - val_loss: 4.9069 - val_accuracy: 0.6113 - 720ms/epoch - 9ms/step\n",
      "Epoch 194/256\n",
      "79/79 - 1s - loss: 1.6180e-07 - accuracy: 1.0000 - val_loss: 4.9189 - val_accuracy: 0.6123 - 667ms/epoch - 8ms/step\n",
      "Epoch 195/256\n",
      "79/79 - 1s - loss: 1.5562e-07 - accuracy: 1.0000 - val_loss: 4.9301 - val_accuracy: 0.6107 - 680ms/epoch - 9ms/step\n",
      "Epoch 196/256\n",
      "79/79 - 1s - loss: 1.5013e-07 - accuracy: 1.0000 - val_loss: 4.9398 - val_accuracy: 0.6113 - 713ms/epoch - 9ms/step\n",
      "Epoch 197/256\n",
      "79/79 - 1s - loss: 1.4267e-07 - accuracy: 1.0000 - val_loss: 4.9505 - val_accuracy: 0.6127 - 724ms/epoch - 9ms/step\n",
      "Epoch 198/256\n",
      "79/79 - 1s - loss: 1.3700e-07 - accuracy: 1.0000 - val_loss: 4.9660 - val_accuracy: 0.6120 - 663ms/epoch - 8ms/step\n",
      "Epoch 199/256\n",
      "79/79 - 1s - loss: 1.3201e-07 - accuracy: 1.0000 - val_loss: 4.9720 - val_accuracy: 0.6103 - 661ms/epoch - 8ms/step\n",
      "Epoch 200/256\n",
      "79/79 - 1s - loss: 1.3289e-07 - accuracy: 1.0000 - val_loss: 4.9818 - val_accuracy: 0.6110 - 709ms/epoch - 9ms/step\n",
      "Epoch 201/256\n",
      "79/79 - 1s - loss: 1.2078e-07 - accuracy: 1.0000 - val_loss: 4.9949 - val_accuracy: 0.6107 - 665ms/epoch - 8ms/step\n",
      "Epoch 202/256\n",
      "79/79 - 1s - loss: 1.1691e-07 - accuracy: 1.0000 - val_loss: 5.0064 - val_accuracy: 0.6107 - 669ms/epoch - 8ms/step\n",
      "Epoch 203/256\n",
      "79/79 - 1s - loss: 1.1153e-07 - accuracy: 1.0000 - val_loss: 5.0160 - val_accuracy: 0.6110 - 681ms/epoch - 9ms/step\n",
      "Epoch 204/256\n",
      "79/79 - 1s - loss: 1.0730e-07 - accuracy: 1.0000 - val_loss: 5.0304 - val_accuracy: 0.6100 - 709ms/epoch - 9ms/step\n",
      "Epoch 205/256\n",
      "79/79 - 1s - loss: 1.0303e-07 - accuracy: 1.0000 - val_loss: 5.0403 - val_accuracy: 0.6110 - 672ms/epoch - 9ms/step\n",
      "Epoch 206/256\n",
      "79/79 - 1s - loss: 9.8443e-08 - accuracy: 1.0000 - val_loss: 5.0486 - val_accuracy: 0.6107 - 677ms/epoch - 9ms/step\n",
      "Epoch 207/256\n",
      "79/79 - 1s - loss: 9.4593e-08 - accuracy: 1.0000 - val_loss: 5.0579 - val_accuracy: 0.6123 - 668ms/epoch - 8ms/step\n",
      "Epoch 208/256\n",
      "79/79 - 1s - loss: 9.0480e-08 - accuracy: 1.0000 - val_loss: 5.0741 - val_accuracy: 0.6117 - 721ms/epoch - 9ms/step\n",
      "Epoch 209/256\n",
      "79/79 - 1s - loss: 8.6713e-08 - accuracy: 1.0000 - val_loss: 5.0860 - val_accuracy: 0.6110 - 668ms/epoch - 8ms/step\n",
      "Epoch 210/256\n",
      "79/79 - 1s - loss: 8.2910e-08 - accuracy: 1.0000 - val_loss: 5.0974 - val_accuracy: 0.6093 - 668ms/epoch - 8ms/step\n",
      "Epoch 211/256\n",
      "79/79 - 1s - loss: 7.9608e-08 - accuracy: 1.0000 - val_loss: 5.1057 - val_accuracy: 0.6117 - 668ms/epoch - 8ms/step\n",
      "Epoch 212/256\n",
      "79/79 - 1s - loss: 7.6342e-08 - accuracy: 1.0000 - val_loss: 5.1198 - val_accuracy: 0.6107 - 726ms/epoch - 9ms/step\n",
      "Epoch 213/256\n",
      "79/79 - 1s - loss: 7.3898e-08 - accuracy: 1.0000 - val_loss: 5.1317 - val_accuracy: 0.6103 - 676ms/epoch - 9ms/step\n",
      "Epoch 214/256\n",
      "79/79 - 1s - loss: 7.0393e-08 - accuracy: 1.0000 - val_loss: 5.1349 - val_accuracy: 0.6107 - 677ms/epoch - 9ms/step\n",
      "Epoch 215/256\n",
      "79/79 - 1s - loss: 6.7925e-08 - accuracy: 1.0000 - val_loss: 5.1495 - val_accuracy: 0.6107 - 716ms/epoch - 9ms/step\n",
      "Epoch 216/256\n",
      "79/79 - 1s - loss: 6.4719e-08 - accuracy: 1.0000 - val_loss: 5.1674 - val_accuracy: 0.6087 - 668ms/epoch - 8ms/step\n",
      "Epoch 217/256\n",
      "79/79 - 1s - loss: 6.2323e-08 - accuracy: 1.0000 - val_loss: 5.1665 - val_accuracy: 0.6100 - 676ms/epoch - 9ms/step\n",
      "Epoch 218/256\n",
      "79/79 - 1s - loss: 5.9485e-08 - accuracy: 1.0000 - val_loss: 5.1837 - val_accuracy: 0.6110 - 662ms/epoch - 8ms/step\n",
      "Epoch 219/256\n",
      "79/79 - 1s - loss: 5.7030e-08 - accuracy: 1.0000 - val_loss: 5.1869 - val_accuracy: 0.6113 - 665ms/epoch - 8ms/step\n",
      "Epoch 220/256\n",
      "79/79 - 1s - loss: 5.4336e-08 - accuracy: 1.0000 - val_loss: 5.2040 - val_accuracy: 0.6107 - 719ms/epoch - 9ms/step\n",
      "Epoch 221/256\n",
      "79/79 - 1s - loss: 5.2536e-08 - accuracy: 1.0000 - val_loss: 5.2133 - val_accuracy: 0.6100 - 690ms/epoch - 9ms/step\n",
      "Epoch 222/256\n",
      "79/79 - 1s - loss: 5.0199e-08 - accuracy: 1.0000 - val_loss: 5.2270 - val_accuracy: 0.6100 - 705ms/epoch - 9ms/step\n",
      "Epoch 223/256\n",
      "79/79 - 1s - loss: 4.8184e-08 - accuracy: 1.0000 - val_loss: 5.2388 - val_accuracy: 0.6097 - 716ms/epoch - 9ms/step\n",
      "Epoch 224/256\n",
      "79/79 - 1s - loss: 4.6301e-08 - accuracy: 1.0000 - val_loss: 5.2472 - val_accuracy: 0.6103 - 679ms/epoch - 9ms/step\n",
      "Epoch 225/256\n",
      "79/79 - 1s - loss: 4.4429e-08 - accuracy: 1.0000 - val_loss: 5.2514 - val_accuracy: 0.6100 - 712ms/epoch - 9ms/step\n",
      "Epoch 226/256\n",
      "79/79 - 1s - loss: 4.2701e-08 - accuracy: 1.0000 - val_loss: 5.2675 - val_accuracy: 0.6093 - 706ms/epoch - 9ms/step\n",
      "Epoch 227/256\n",
      "79/79 - 1s - loss: 4.1103e-08 - accuracy: 1.0000 - val_loss: 5.2752 - val_accuracy: 0.6097 - 678ms/epoch - 9ms/step\n",
      "Epoch 228/256\n",
      "79/79 - 1s - loss: 3.9363e-08 - accuracy: 1.0000 - val_loss: 5.2871 - val_accuracy: 0.6097 - 665ms/epoch - 8ms/step\n",
      "Epoch 229/256\n",
      "79/79 - 1s - loss: 3.7575e-08 - accuracy: 1.0000 - val_loss: 5.2977 - val_accuracy: 0.6097 - 724ms/epoch - 9ms/step\n",
      "Epoch 230/256\n",
      "79/79 - 1s - loss: 3.6240e-08 - accuracy: 1.0000 - val_loss: 5.3128 - val_accuracy: 0.6100 - 724ms/epoch - 9ms/step\n",
      "Epoch 231/256\n",
      "79/79 - 1s - loss: 3.5000e-08 - accuracy: 1.0000 - val_loss: 5.3237 - val_accuracy: 0.6113 - 664ms/epoch - 8ms/step\n",
      "Epoch 232/256\n",
      "79/79 - 1s - loss: 3.3367e-08 - accuracy: 1.0000 - val_loss: 5.3336 - val_accuracy: 0.6097 - 661ms/epoch - 8ms/step\n",
      "Epoch 233/256\n",
      "79/79 - 1s - loss: 3.1924e-08 - accuracy: 1.0000 - val_loss: 5.3500 - val_accuracy: 0.6097 - 667ms/epoch - 8ms/step\n",
      "Epoch 234/256\n",
      "79/79 - 1s - loss: 3.1173e-08 - accuracy: 1.0000 - val_loss: 5.3615 - val_accuracy: 0.6103 - 662ms/epoch - 8ms/step\n",
      "Epoch 235/256\n",
      "79/79 - 1s - loss: 2.9945e-08 - accuracy: 1.0000 - val_loss: 5.3728 - val_accuracy: 0.6100 - 707ms/epoch - 9ms/step\n",
      "Epoch 236/256\n",
      "79/79 - 1s - loss: 2.8384e-08 - accuracy: 1.0000 - val_loss: 5.3865 - val_accuracy: 0.6087 - 671ms/epoch - 8ms/step\n",
      "Epoch 237/256\n",
      "79/79 - 1s - loss: 2.7537e-08 - accuracy: 1.0000 - val_loss: 5.4032 - val_accuracy: 0.6087 - 717ms/epoch - 9ms/step\n",
      "Epoch 238/256\n",
      "79/79 - 1s - loss: 2.6810e-08 - accuracy: 1.0000 - val_loss: 5.4141 - val_accuracy: 0.6087 - 665ms/epoch - 8ms/step\n",
      "Epoch 239/256\n",
      "79/79 - 1s - loss: 2.6250e-08 - accuracy: 1.0000 - val_loss: 5.4268 - val_accuracy: 0.6107 - 662ms/epoch - 8ms/step\n",
      "Epoch 240/256\n",
      "79/79 - 1s - loss: 2.5201e-08 - accuracy: 1.0000 - val_loss: 5.4428 - val_accuracy: 0.6107 - 669ms/epoch - 8ms/step\n",
      "Epoch 241/256\n",
      "79/79 - 1s - loss: 2.4450e-08 - accuracy: 1.0000 - val_loss: 5.4563 - val_accuracy: 0.6093 - 708ms/epoch - 9ms/step\n",
      "Epoch 242/256\n",
      "79/79 - 1s - loss: 2.3115e-08 - accuracy: 1.0000 - val_loss: 5.4742 - val_accuracy: 0.6100 - 718ms/epoch - 9ms/step\n",
      "Epoch 243/256\n",
      "79/79 - 1s - loss: 2.2626e-08 - accuracy: 1.0000 - val_loss: 5.4871 - val_accuracy: 0.6097 - 715ms/epoch - 9ms/step\n",
      "Epoch 244/256\n",
      "79/79 - 1s - loss: 2.1815e-08 - accuracy: 1.0000 - val_loss: 5.5049 - val_accuracy: 0.6090 - 718ms/epoch - 9ms/step\n",
      "Epoch 245/256\n",
      "79/79 - 1s - loss: 2.1327e-08 - accuracy: 1.0000 - val_loss: 5.5234 - val_accuracy: 0.6100 - 800ms/epoch - 10ms/step\n",
      "Epoch 246/256\n",
      "79/79 - 1s - loss: 2.0659e-08 - accuracy: 1.0000 - val_loss: 5.5375 - val_accuracy: 0.6087 - 920ms/epoch - 12ms/step\n",
      "Epoch 247/256\n",
      "79/79 - 1s - loss: 2.0325e-08 - accuracy: 1.0000 - val_loss: 5.5595 - val_accuracy: 0.6110 - 931ms/epoch - 12ms/step\n",
      "Epoch 248/256\n",
      "79/79 - 1s - loss: 1.9479e-08 - accuracy: 1.0000 - val_loss: 5.5788 - val_accuracy: 0.6090 - 722ms/epoch - 9ms/step\n",
      "Epoch 249/256\n",
      "79/79 - 1s - loss: 1.9181e-08 - accuracy: 1.0000 - val_loss: 5.6046 - val_accuracy: 0.6090 - 720ms/epoch - 9ms/step\n",
      "Epoch 250/256\n",
      "79/79 - 1s - loss: 1.8668e-08 - accuracy: 1.0000 - val_loss: 5.6242 - val_accuracy: 0.6083 - 714ms/epoch - 9ms/step\n",
      "Epoch 251/256\n",
      "79/79 - 1s - loss: 1.8454e-08 - accuracy: 1.0000 - val_loss: 5.6422 - val_accuracy: 0.6090 - 680ms/epoch - 9ms/step\n",
      "Epoch 252/256\n",
      "79/79 - 1s - loss: 1.7834e-08 - accuracy: 1.0000 - val_loss: 5.6655 - val_accuracy: 0.6090 - 687ms/epoch - 9ms/step\n",
      "Epoch 253/256\n",
      "79/79 - 1s - loss: 1.7726e-08 - accuracy: 1.0000 - val_loss: 5.6933 - val_accuracy: 0.6073 - 696ms/epoch - 9ms/step\n",
      "Epoch 254/256\n",
      "79/79 - 1s - loss: 1.7655e-08 - accuracy: 1.0000 - val_loss: 5.7152 - val_accuracy: 0.6073 - 955ms/epoch - 12ms/step\n",
      "Epoch 255/256\n",
      "79/79 - 1s - loss: 1.6880e-08 - accuracy: 1.0000 - val_loss: 5.7399 - val_accuracy: 0.6090 - 979ms/epoch - 12ms/step\n",
      "Epoch 256/256\n",
      "79/79 - 1s - loss: 1.6844e-08 - accuracy: 1.0000 - val_loss: 5.7655 - val_accuracy: 0.6077 - 728ms/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f48165bf810>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=Adam(lr=0.001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
    "\n",
    "model.fit( x_train[0:10000], y_train[0:10000],\n",
    "          batch_size=128, epochs=256, verbose=2,\n",
    "          validation_data=(x_test[0:3000], y_test[0:3000]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX5QhfY7OyAg"
   },
   "source": [
    "### Write your code in the cell below to print out the Precision, Recall,  F1 score, and classification_*report*\n",
    "\n",
    "### Include your findings in the project report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Stb4cAuROzqF",
    "outputId": "6b7ebcd4-af35-4aa2-a541-64884fef6a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step\n",
      "Accuracy Score : 0.616\n",
      "Averaged Precision Score : 0.6143961153939752\n",
      "Averaged Recall Score : 0.616\n",
      "Averaged F1 Score : 0.6144336583089134\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67       299\n",
      "           1       0.71      0.70      0.70       287\n",
      "           2       0.52      0.50      0.51       318\n",
      "           3       0.42      0.43      0.42       295\n",
      "           4       0.54      0.55      0.55       299\n",
      "           5       0.51      0.45      0.48       290\n",
      "           6       0.69      0.76      0.72       307\n",
      "           7       0.70      0.68      0.69       286\n",
      "           8       0.68      0.77      0.72       316\n",
      "           9       0.71      0.66      0.68       303\n",
      "\n",
      "    accuracy                           0.62      3000\n",
      "   macro avg       0.61      0.62      0.61      3000\n",
      "weighted avg       0.61      0.62      0.61      3000\n",
      "\n",
      "[5.790398597717285, 0.6159999966621399]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = np.argmax(y_test[0:3000], axis=1)\n",
    "prediction = model.predict(x_test[0:3000])\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "\n",
    "metrics = { 'Accuracy Score' : metrics.accuracy_score(y_true, prediction),\n",
    "            'Averaged Precision Score' : metrics.precision_score(y_true, prediction, average='weighted'),\n",
    "            'Averaged Recall Score' : metrics.recall_score(y_true, prediction, average='weighted'),\n",
    "            'Averaged F1 Score' : metrics.f1_score(y_true, prediction, average='weighted')}\n",
    "for k,v in metrics.items():\n",
    "  print(f'{k} : {v}')\n",
    "\n",
    "print(classification_report(y_true, prediction))\n",
    "print( model.evaluate(x_test[0:3000], y_test[0:3000], verbose=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4geLFKNmO3NT"
   },
   "source": [
    "### Write your code in the cell below to show 3-5 images in the test set as well as their true labels and their labels predicted by your model.\n",
    "\n",
    "### Include your findings in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3rPXHVsO8xb",
    "outputId": "6e008a17-bc17-4365-e713-3faa04894bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photograph #1143... Actual Classification: 6 -> Predicted Classification: 6\n",
      "Photograph #733... Actual Classification: 9 -> Predicted Classification: 9\n",
      "Photograph #1575... Actual Classification: 2 -> Predicted Classification: 2\n",
      "Photograph #1779... Actual Classification: 3 -> Predicted Classification: 5\n",
      "Photograph #2367... Actual Classification: 4 -> Predicted Classification: 2\n",
      "Photograph #2066... Actual Classification: 2 -> Predicted Classification: 2\n",
      "Photograph #2836... Actual Classification: 3 -> Predicted Classification: 3\n",
      "Photograph #492... Actual Classification: 8 -> Predicted Classification: 8\n",
      "Photograph #1252... Actual Classification: 5 -> Predicted Classification: 3\n",
      "Photograph #1302... Actual Classification: 6 -> Predicted Classification: 6\n",
      "Photograph #658... Actual Classification: 4 -> Predicted Classification: 4\n",
      "Photograph #125... Actual Classification: 0 -> Predicted Classification: 9\n",
      "Photograph #1899... Actual Classification: 7 -> Predicted Classification: 7\n",
      "Photograph #36... Actual Classification: 4 -> Predicted Classification: 7\n",
      "Photograph #591... Actual Classification: 9 -> Predicted Classification: 9\n",
      "Photograph #2563... Actual Classification: 2 -> Predicted Classification: 2\n",
      "Photograph #384... Actual Classification: 2 -> Predicted Classification: 2\n",
      "Photograph #740... Actual Classification: 2 -> Predicted Classification: 5\n",
      "Photograph #647... Actual Classification: 4 -> Predicted Classification: 3\n",
      "Photograph #1374... Actual Classification: 2 -> Predicted Classification: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "  idx = np.random.randint(1,prediction.shape[0])\n",
    "  print(f'Photograph #{idx}... Actual Classification: {y_true[idx]} -> Predicted Classification: {prediction[idx]}')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiPcA3tLvL6b"
   },
   "source": [
    "Test Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8r9O4KquM0s",
    "outputId": "b46322d7-6164-436e-dafa-179f1629b7e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 512)               2097664   \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,122,186\n",
      "Trainable params: 2,122,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                 padding='same', input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.40))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKVF0zNuw9Hw",
    "outputId": "e4872419-1700-4595-87bd-a983aa47341f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "149/149 - 2s - loss: 1.8246 - accuracy: 0.3364 - val_loss: 1.4858 - val_accuracy: 0.4692 - 2s/epoch - 14ms/step\n",
      "Epoch 2/256\n",
      "149/149 - 1s - loss: 1.4359 - accuracy: 0.4823 - val_loss: 1.3209 - val_accuracy: 0.5296 - 1s/epoch - 8ms/step\n",
      "Epoch 3/256\n",
      "149/149 - 1s - loss: 1.2780 - accuracy: 0.5415 - val_loss: 1.1906 - val_accuracy: 0.5834 - 1s/epoch - 7ms/step\n",
      "Epoch 4/256\n",
      "149/149 - 1s - loss: 1.1662 - accuracy: 0.5811 - val_loss: 1.1213 - val_accuracy: 0.6038 - 1s/epoch - 8ms/step\n",
      "Epoch 5/256\n",
      "149/149 - 1s - loss: 1.0851 - accuracy: 0.6123 - val_loss: 1.0372 - val_accuracy: 0.6372 - 1s/epoch - 7ms/step\n",
      "Epoch 6/256\n",
      "149/149 - 1s - loss: 1.0173 - accuracy: 0.6392 - val_loss: 1.0208 - val_accuracy: 0.6396 - 1s/epoch - 8ms/step\n",
      "Epoch 7/256\n",
      "149/149 - 1s - loss: 0.9513 - accuracy: 0.6622 - val_loss: 0.9928 - val_accuracy: 0.6542 - 1s/epoch - 8ms/step\n",
      "Epoch 8/256\n",
      "149/149 - 1s - loss: 0.9049 - accuracy: 0.6773 - val_loss: 0.9597 - val_accuracy: 0.6718 - 1s/epoch - 7ms/step\n",
      "Epoch 9/256\n",
      "149/149 - 1s - loss: 0.8430 - accuracy: 0.6981 - val_loss: 0.9627 - val_accuracy: 0.6672 - 1s/epoch - 7ms/step\n",
      "Epoch 10/256\n",
      "149/149 - 1s - loss: 0.7989 - accuracy: 0.7188 - val_loss: 0.9393 - val_accuracy: 0.6766 - 1s/epoch - 7ms/step\n",
      "Epoch 11/256\n",
      "149/149 - 1s - loss: 0.7569 - accuracy: 0.7348 - val_loss: 0.9122 - val_accuracy: 0.6882 - 1s/epoch - 7ms/step\n",
      "Epoch 12/256\n",
      "149/149 - 1s - loss: 0.7115 - accuracy: 0.7484 - val_loss: 0.9203 - val_accuracy: 0.6852 - 1s/epoch - 7ms/step\n",
      "Epoch 13/256\n",
      "149/149 - 1s - loss: 0.6664 - accuracy: 0.7673 - val_loss: 0.8999 - val_accuracy: 0.6934 - 1s/epoch - 7ms/step\n",
      "Epoch 14/256\n",
      "149/149 - 1s - loss: 0.6294 - accuracy: 0.7779 - val_loss: 0.9086 - val_accuracy: 0.6892 - 1s/epoch - 7ms/step\n",
      "Epoch 15/256\n",
      "149/149 - 1s - loss: 0.5940 - accuracy: 0.7908 - val_loss: 0.9033 - val_accuracy: 0.6978 - 1s/epoch - 7ms/step\n",
      "Epoch 16/256\n",
      "149/149 - 1s - loss: 0.5610 - accuracy: 0.7998 - val_loss: 0.9228 - val_accuracy: 0.6942 - 1s/epoch - 7ms/step\n",
      "Epoch 17/256\n",
      "149/149 - 1s - loss: 0.5230 - accuracy: 0.8119 - val_loss: 0.9199 - val_accuracy: 0.6992 - 1s/epoch - 7ms/step\n",
      "Epoch 18/256\n",
      "149/149 - 1s - loss: 0.4805 - accuracy: 0.8305 - val_loss: 0.9288 - val_accuracy: 0.6970 - 1s/epoch - 7ms/step\n",
      "Epoch 19/256\n",
      "149/149 - 1s - loss: 0.4561 - accuracy: 0.8383 - val_loss: 0.9325 - val_accuracy: 0.7040 - 1s/epoch - 7ms/step\n",
      "Epoch 20/256\n",
      "149/149 - 1s - loss: 0.4288 - accuracy: 0.8490 - val_loss: 0.9658 - val_accuracy: 0.6988 - 1s/epoch - 7ms/step\n",
      "Epoch 21/256\n",
      "149/149 - 1s - loss: 0.3982 - accuracy: 0.8614 - val_loss: 0.9533 - val_accuracy: 0.7088 - 1s/epoch - 7ms/step\n",
      "Epoch 22/256\n",
      "149/149 - 1s - loss: 0.3839 - accuracy: 0.8625 - val_loss: 0.9723 - val_accuracy: 0.7074 - 1s/epoch - 7ms/step\n",
      "Epoch 23/256\n",
      "149/149 - 1s - loss: 0.3589 - accuracy: 0.8713 - val_loss: 0.9991 - val_accuracy: 0.7020 - 1s/epoch - 7ms/step\n",
      "Epoch 24/256\n",
      "149/149 - 1s - loss: 0.3434 - accuracy: 0.8793 - val_loss: 1.0105 - val_accuracy: 0.6946 - 1s/epoch - 7ms/step\n",
      "Epoch 25/256\n",
      "149/149 - 1s - loss: 0.3177 - accuracy: 0.8873 - val_loss: 0.9909 - val_accuracy: 0.6962 - 1s/epoch - 7ms/step\n",
      "Epoch 26/256\n",
      "149/149 - 1s - loss: 0.3135 - accuracy: 0.8883 - val_loss: 1.0159 - val_accuracy: 0.7040 - 1s/epoch - 7ms/step\n",
      "Epoch 27/256\n",
      "149/149 - 1s - loss: 0.2800 - accuracy: 0.9036 - val_loss: 1.0286 - val_accuracy: 0.7042 - 1s/epoch - 7ms/step\n",
      "Epoch 28/256\n",
      "149/149 - 1s - loss: 0.2770 - accuracy: 0.9024 - val_loss: 1.0591 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 29/256\n",
      "149/149 - 1s - loss: 0.2622 - accuracy: 0.9064 - val_loss: 1.0682 - val_accuracy: 0.6996 - 1s/epoch - 7ms/step\n",
      "Epoch 30/256\n",
      "149/149 - 1s - loss: 0.2611 - accuracy: 0.9088 - val_loss: 1.1005 - val_accuracy: 0.6942 - 1s/epoch - 7ms/step\n",
      "Epoch 31/256\n",
      "149/149 - 1s - loss: 0.2273 - accuracy: 0.9198 - val_loss: 1.0933 - val_accuracy: 0.6956 - 1s/epoch - 7ms/step\n",
      "Epoch 32/256\n",
      "149/149 - 1s - loss: 0.2298 - accuracy: 0.9183 - val_loss: 1.1125 - val_accuracy: 0.6978 - 1s/epoch - 8ms/step\n",
      "Epoch 33/256\n",
      "149/149 - 1s - loss: 0.2211 - accuracy: 0.9227 - val_loss: 1.1021 - val_accuracy: 0.7030 - 1s/epoch - 7ms/step\n",
      "Epoch 34/256\n",
      "149/149 - 1s - loss: 0.2176 - accuracy: 0.9241 - val_loss: 1.1391 - val_accuracy: 0.6956 - 1s/epoch - 7ms/step\n",
      "Epoch 35/256\n",
      "149/149 - 1s - loss: 0.2078 - accuracy: 0.9284 - val_loss: 1.1667 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 36/256\n",
      "149/149 - 1s - loss: 0.1996 - accuracy: 0.9308 - val_loss: 1.1429 - val_accuracy: 0.6962 - 1s/epoch - 8ms/step\n",
      "Epoch 37/256\n",
      "149/149 - 1s - loss: 0.1938 - accuracy: 0.9333 - val_loss: 1.1657 - val_accuracy: 0.7032 - 1s/epoch - 7ms/step\n",
      "Epoch 38/256\n",
      "149/149 - 1s - loss: 0.1780 - accuracy: 0.9402 - val_loss: 1.1820 - val_accuracy: 0.6990 - 1s/epoch - 7ms/step\n",
      "Epoch 39/256\n",
      "149/149 - 1s - loss: 0.1847 - accuracy: 0.9355 - val_loss: 1.1954 - val_accuracy: 0.7012 - 1s/epoch - 7ms/step\n",
      "Epoch 40/256\n",
      "149/149 - 1s - loss: 0.1742 - accuracy: 0.9414 - val_loss: 1.1847 - val_accuracy: 0.6990 - 1s/epoch - 7ms/step\n",
      "Epoch 41/256\n",
      "149/149 - 1s - loss: 0.1694 - accuracy: 0.9401 - val_loss: 1.2081 - val_accuracy: 0.6974 - 1s/epoch - 7ms/step\n",
      "Epoch 42/256\n",
      "149/149 - 1s - loss: 0.1654 - accuracy: 0.9426 - val_loss: 1.2522 - val_accuracy: 0.6874 - 1s/epoch - 7ms/step\n",
      "Epoch 43/256\n",
      "149/149 - 1s - loss: 0.1568 - accuracy: 0.9455 - val_loss: 1.2533 - val_accuracy: 0.7000 - 1s/epoch - 7ms/step\n",
      "Epoch 44/256\n",
      "149/149 - 1s - loss: 0.1697 - accuracy: 0.9402 - val_loss: 1.2408 - val_accuracy: 0.7004 - 1s/epoch - 7ms/step\n",
      "Epoch 45/256\n",
      "149/149 - 1s - loss: 0.1567 - accuracy: 0.9463 - val_loss: 1.2730 - val_accuracy: 0.7010 - 1s/epoch - 7ms/step\n",
      "Epoch 46/256\n",
      "149/149 - 1s - loss: 0.1565 - accuracy: 0.9457 - val_loss: 1.2438 - val_accuracy: 0.6944 - 1s/epoch - 7ms/step\n",
      "Epoch 47/256\n",
      "149/149 - 1s - loss: 0.1389 - accuracy: 0.9516 - val_loss: 1.3335 - val_accuracy: 0.7018 - 1s/epoch - 7ms/step\n",
      "Epoch 48/256\n",
      "149/149 - 1s - loss: 0.1511 - accuracy: 0.9464 - val_loss: 1.2898 - val_accuracy: 0.7024 - 1s/epoch - 7ms/step\n",
      "Epoch 49/256\n",
      "149/149 - 1s - loss: 0.1376 - accuracy: 0.9514 - val_loss: 1.3030 - val_accuracy: 0.6994 - 1s/epoch - 7ms/step\n",
      "Epoch 50/256\n",
      "149/149 - 1s - loss: 0.1362 - accuracy: 0.9523 - val_loss: 1.3503 - val_accuracy: 0.6970 - 1s/epoch - 7ms/step\n",
      "Epoch 51/256\n",
      "149/149 - 1s - loss: 0.1338 - accuracy: 0.9548 - val_loss: 1.3170 - val_accuracy: 0.7018 - 1s/epoch - 7ms/step\n",
      "Epoch 52/256\n",
      "149/149 - 1s - loss: 0.1338 - accuracy: 0.9537 - val_loss: 1.4101 - val_accuracy: 0.6904 - 1s/epoch - 7ms/step\n",
      "Epoch 53/256\n",
      "149/149 - 1s - loss: 0.1315 - accuracy: 0.9530 - val_loss: 1.3718 - val_accuracy: 0.6976 - 1s/epoch - 7ms/step\n",
      "Epoch 54/256\n",
      "149/149 - 1s - loss: 0.1330 - accuracy: 0.9523 - val_loss: 1.3701 - val_accuracy: 0.7002 - 1s/epoch - 7ms/step\n",
      "Epoch 55/256\n",
      "149/149 - 1s - loss: 0.1292 - accuracy: 0.9558 - val_loss: 1.4246 - val_accuracy: 0.6978 - 1s/epoch - 7ms/step\n",
      "Epoch 56/256\n",
      "149/149 - 1s - loss: 0.1226 - accuracy: 0.9577 - val_loss: 1.4070 - val_accuracy: 0.7006 - 1s/epoch - 7ms/step\n",
      "Epoch 57/256\n",
      "149/149 - 1s - loss: 0.1210 - accuracy: 0.9587 - val_loss: 1.3858 - val_accuracy: 0.6938 - 1s/epoch - 9ms/step\n",
      "Epoch 58/256\n",
      "149/149 - 1s - loss: 0.1201 - accuracy: 0.9594 - val_loss: 1.3581 - val_accuracy: 0.6968 - 1s/epoch - 9ms/step\n",
      "Epoch 59/256\n",
      "149/149 - 1s - loss: 0.1109 - accuracy: 0.9626 - val_loss: 1.4361 - val_accuracy: 0.6946 - 1s/epoch - 8ms/step\n",
      "Epoch 60/256\n",
      "149/149 - 2s - loss: 0.1104 - accuracy: 0.9615 - val_loss: 1.3959 - val_accuracy: 0.7032 - 2s/epoch - 14ms/step\n",
      "Epoch 61/256\n",
      "149/149 - 2s - loss: 0.1118 - accuracy: 0.9622 - val_loss: 1.4269 - val_accuracy: 0.7014 - 2s/epoch - 11ms/step\n",
      "Epoch 62/256\n",
      "149/149 - 1s - loss: 0.1133 - accuracy: 0.9601 - val_loss: 1.4013 - val_accuracy: 0.6996 - 1s/epoch - 7ms/step\n",
      "Epoch 63/256\n",
      "149/149 - 1s - loss: 0.1145 - accuracy: 0.9620 - val_loss: 1.4251 - val_accuracy: 0.6968 - 1s/epoch - 7ms/step\n",
      "Epoch 64/256\n",
      "149/149 - 1s - loss: 0.1075 - accuracy: 0.9622 - val_loss: 1.4911 - val_accuracy: 0.6912 - 1s/epoch - 7ms/step\n",
      "Epoch 65/256\n",
      "149/149 - 1s - loss: 0.1142 - accuracy: 0.9617 - val_loss: 1.4380 - val_accuracy: 0.6968 - 1s/epoch - 7ms/step\n",
      "Epoch 66/256\n",
      "149/149 - 1s - loss: 0.1044 - accuracy: 0.9654 - val_loss: 1.4493 - val_accuracy: 0.6898 - 1s/epoch - 7ms/step\n",
      "Epoch 67/256\n",
      "149/149 - 1s - loss: 0.1011 - accuracy: 0.9659 - val_loss: 1.4907 - val_accuracy: 0.6970 - 1s/epoch - 7ms/step\n",
      "Epoch 68/256\n",
      "149/149 - 1s - loss: 0.1033 - accuracy: 0.9639 - val_loss: 1.5248 - val_accuracy: 0.6940 - 1s/epoch - 7ms/step\n",
      "Epoch 69/256\n",
      "149/149 - 1s - loss: 0.0962 - accuracy: 0.9658 - val_loss: 1.4679 - val_accuracy: 0.6970 - 1s/epoch - 7ms/step\n",
      "Epoch 70/256\n",
      "149/149 - 1s - loss: 0.0988 - accuracy: 0.9646 - val_loss: 1.5849 - val_accuracy: 0.6930 - 1s/epoch - 7ms/step\n",
      "Epoch 71/256\n",
      "149/149 - 1s - loss: 0.1058 - accuracy: 0.9640 - val_loss: 1.5209 - val_accuracy: 0.7004 - 1s/epoch - 7ms/step\n",
      "Epoch 72/256\n",
      "149/149 - 1s - loss: 0.0944 - accuracy: 0.9692 - val_loss: 1.4898 - val_accuracy: 0.6972 - 1s/epoch - 7ms/step\n",
      "Epoch 73/256\n",
      "149/149 - 1s - loss: 0.0962 - accuracy: 0.9653 - val_loss: 1.5085 - val_accuracy: 0.7014 - 1s/epoch - 7ms/step\n",
      "Epoch 74/256\n",
      "149/149 - 1s - loss: 0.1024 - accuracy: 0.9649 - val_loss: 1.5136 - val_accuracy: 0.7000 - 1s/epoch - 8ms/step\n",
      "Epoch 75/256\n",
      "149/149 - 1s - loss: 0.0975 - accuracy: 0.9673 - val_loss: 1.5466 - val_accuracy: 0.6952 - 1s/epoch - 7ms/step\n",
      "Epoch 76/256\n",
      "149/149 - 1s - loss: 0.0939 - accuracy: 0.9683 - val_loss: 1.5160 - val_accuracy: 0.7004 - 1s/epoch - 7ms/step\n",
      "Epoch 77/256\n",
      "149/149 - 1s - loss: 0.0937 - accuracy: 0.9688 - val_loss: 1.4860 - val_accuracy: 0.6980 - 1s/epoch - 7ms/step\n",
      "Epoch 78/256\n",
      "149/149 - 1s - loss: 0.0947 - accuracy: 0.9661 - val_loss: 1.5328 - val_accuracy: 0.7032 - 1s/epoch - 7ms/step\n",
      "Epoch 79/256\n",
      "149/149 - 1s - loss: 0.0920 - accuracy: 0.9684 - val_loss: 1.5674 - val_accuracy: 0.7012 - 1s/epoch - 7ms/step\n",
      "Epoch 80/256\n",
      "149/149 - 1s - loss: 0.0845 - accuracy: 0.9716 - val_loss: 1.5997 - val_accuracy: 0.6940 - 1s/epoch - 7ms/step\n",
      "Epoch 81/256\n",
      "149/149 - 1s - loss: 0.0897 - accuracy: 0.9692 - val_loss: 1.6089 - val_accuracy: 0.6872 - 1s/epoch - 7ms/step\n",
      "Epoch 82/256\n",
      "149/149 - 1s - loss: 0.0892 - accuracy: 0.9699 - val_loss: 1.5959 - val_accuracy: 0.6948 - 1s/epoch - 7ms/step\n",
      "Epoch 83/256\n",
      "149/149 - 1s - loss: 0.0888 - accuracy: 0.9694 - val_loss: 1.6014 - val_accuracy: 0.7010 - 1s/epoch - 7ms/step\n",
      "Epoch 84/256\n",
      "149/149 - 1s - loss: 0.0948 - accuracy: 0.9669 - val_loss: 1.5503 - val_accuracy: 0.7002 - 1s/epoch - 7ms/step\n",
      "Epoch 85/256\n",
      "149/149 - 1s - loss: 0.0875 - accuracy: 0.9694 - val_loss: 1.5516 - val_accuracy: 0.6972 - 1s/epoch - 7ms/step\n",
      "Epoch 86/256\n",
      "149/149 - 1s - loss: 0.0860 - accuracy: 0.9706 - val_loss: 1.5530 - val_accuracy: 0.7046 - 1s/epoch - 7ms/step\n",
      "Epoch 87/256\n",
      "149/149 - 1s - loss: 0.0781 - accuracy: 0.9738 - val_loss: 1.6737 - val_accuracy: 0.7028 - 1s/epoch - 7ms/step\n",
      "Epoch 88/256\n",
      "149/149 - 1s - loss: 0.0883 - accuracy: 0.9703 - val_loss: 1.6332 - val_accuracy: 0.6966 - 1s/epoch - 7ms/step\n",
      "Epoch 89/256\n",
      "149/149 - 1s - loss: 0.0863 - accuracy: 0.9704 - val_loss: 1.6396 - val_accuracy: 0.7034 - 1s/epoch - 7ms/step\n",
      "Epoch 90/256\n",
      "149/149 - 1s - loss: 0.0780 - accuracy: 0.9752 - val_loss: 1.6211 - val_accuracy: 0.7018 - 1s/epoch - 7ms/step\n",
      "Epoch 91/256\n",
      "149/149 - 1s - loss: 0.0816 - accuracy: 0.9727 - val_loss: 1.6659 - val_accuracy: 0.6998 - 1s/epoch - 8ms/step\n",
      "Epoch 92/256\n",
      "149/149 - 1s - loss: 0.0888 - accuracy: 0.9695 - val_loss: 1.6371 - val_accuracy: 0.6950 - 1s/epoch - 7ms/step\n",
      "Epoch 93/256\n",
      "149/149 - 1s - loss: 0.0803 - accuracy: 0.9719 - val_loss: 1.6583 - val_accuracy: 0.6994 - 1s/epoch - 7ms/step\n",
      "Epoch 94/256\n",
      "149/149 - 1s - loss: 0.0813 - accuracy: 0.9727 - val_loss: 1.6681 - val_accuracy: 0.6944 - 1s/epoch - 7ms/step\n",
      "Epoch 95/256\n",
      "149/149 - 1s - loss: 0.0860 - accuracy: 0.9704 - val_loss: 1.6252 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 96/256\n",
      "149/149 - 1s - loss: 0.0736 - accuracy: 0.9755 - val_loss: 1.6910 - val_accuracy: 0.6988 - 1s/epoch - 7ms/step\n",
      "Epoch 97/256\n",
      "149/149 - 1s - loss: 0.0762 - accuracy: 0.9738 - val_loss: 1.6921 - val_accuracy: 0.7006 - 1s/epoch - 7ms/step\n",
      "Epoch 98/256\n",
      "149/149 - 1s - loss: 0.0797 - accuracy: 0.9732 - val_loss: 1.6928 - val_accuracy: 0.7066 - 1s/epoch - 7ms/step\n",
      "Epoch 99/256\n",
      "149/149 - 1s - loss: 0.0793 - accuracy: 0.9727 - val_loss: 1.6816 - val_accuracy: 0.6952 - 1s/epoch - 7ms/step\n",
      "Epoch 100/256\n",
      "149/149 - 1s - loss: 0.0768 - accuracy: 0.9732 - val_loss: 1.6583 - val_accuracy: 0.7044 - 1s/epoch - 7ms/step\n",
      "Epoch 101/256\n",
      "149/149 - 1s - loss: 0.0802 - accuracy: 0.9721 - val_loss: 1.6600 - val_accuracy: 0.6996 - 1s/epoch - 7ms/step\n",
      "Epoch 102/256\n",
      "149/149 - 1s - loss: 0.0817 - accuracy: 0.9731 - val_loss: 1.7632 - val_accuracy: 0.6920 - 1s/epoch - 7ms/step\n",
      "Epoch 103/256\n",
      "149/149 - 1s - loss: 0.0809 - accuracy: 0.9741 - val_loss: 1.6624 - val_accuracy: 0.6970 - 1s/epoch - 7ms/step\n",
      "Epoch 104/256\n",
      "149/149 - 1s - loss: 0.0767 - accuracy: 0.9739 - val_loss: 1.6354 - val_accuracy: 0.6996 - 1s/epoch - 7ms/step\n",
      "Epoch 105/256\n",
      "149/149 - 1s - loss: 0.0759 - accuracy: 0.9750 - val_loss: 1.7023 - val_accuracy: 0.7036 - 1s/epoch - 7ms/step\n",
      "Epoch 106/256\n",
      "149/149 - 1s - loss: 0.0763 - accuracy: 0.9738 - val_loss: 1.7442 - val_accuracy: 0.6958 - 1s/epoch - 7ms/step\n",
      "Epoch 107/256\n",
      "149/149 - 1s - loss: 0.0740 - accuracy: 0.9743 - val_loss: 1.8036 - val_accuracy: 0.6952 - 1s/epoch - 7ms/step\n",
      "Epoch 108/256\n",
      "149/149 - 1s - loss: 0.0705 - accuracy: 0.9756 - val_loss: 1.7923 - val_accuracy: 0.6964 - 1s/epoch - 7ms/step\n",
      "Epoch 109/256\n",
      "149/149 - 1s - loss: 0.0750 - accuracy: 0.9741 - val_loss: 1.7352 - val_accuracy: 0.7030 - 1s/epoch - 7ms/step\n",
      "Epoch 110/256\n",
      "149/149 - 1s - loss: 0.0789 - accuracy: 0.9729 - val_loss: 1.7778 - val_accuracy: 0.6960 - 1s/epoch - 7ms/step\n",
      "Epoch 111/256\n",
      "149/149 - 1s - loss: 0.0719 - accuracy: 0.9751 - val_loss: 1.8074 - val_accuracy: 0.7018 - 1s/epoch - 7ms/step\n",
      "Epoch 112/256\n",
      "149/149 - 1s - loss: 0.0709 - accuracy: 0.9761 - val_loss: 1.7367 - val_accuracy: 0.6982 - 1s/epoch - 7ms/step\n",
      "Epoch 113/256\n",
      "149/149 - 1s - loss: 0.0724 - accuracy: 0.9743 - val_loss: 1.7269 - val_accuracy: 0.7020 - 1s/epoch - 7ms/step\n",
      "Epoch 114/256\n",
      "149/149 - 1s - loss: 0.0689 - accuracy: 0.9755 - val_loss: 1.8088 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 115/256\n",
      "149/149 - 1s - loss: 0.0690 - accuracy: 0.9752 - val_loss: 1.8936 - val_accuracy: 0.6886 - 1s/epoch - 7ms/step\n",
      "Epoch 116/256\n",
      "149/149 - 1s - loss: 0.0770 - accuracy: 0.9744 - val_loss: 1.8727 - val_accuracy: 0.6876 - 1s/epoch - 7ms/step\n",
      "Epoch 117/256\n",
      "149/149 - 1s - loss: 0.0618 - accuracy: 0.9784 - val_loss: 1.7806 - val_accuracy: 0.6952 - 1s/epoch - 7ms/step\n",
      "Epoch 118/256\n",
      "149/149 - 1s - loss: 0.0695 - accuracy: 0.9773 - val_loss: 1.7831 - val_accuracy: 0.6922 - 1s/epoch - 7ms/step\n",
      "Epoch 119/256\n",
      "149/149 - 1s - loss: 0.0618 - accuracy: 0.9796 - val_loss: 1.8557 - val_accuracy: 0.7014 - 1s/epoch - 7ms/step\n",
      "Epoch 120/256\n",
      "149/149 - 1s - loss: 0.0710 - accuracy: 0.9751 - val_loss: 1.7697 - val_accuracy: 0.6962 - 1s/epoch - 7ms/step\n",
      "Epoch 121/256\n",
      "149/149 - 1s - loss: 0.0746 - accuracy: 0.9764 - val_loss: 1.8037 - val_accuracy: 0.6988 - 1s/epoch - 8ms/step\n",
      "Epoch 122/256\n",
      "149/149 - 1s - loss: 0.0725 - accuracy: 0.9746 - val_loss: 1.8219 - val_accuracy: 0.6898 - 1s/epoch - 7ms/step\n",
      "Epoch 123/256\n",
      "149/149 - 1s - loss: 0.0749 - accuracy: 0.9753 - val_loss: 1.7709 - val_accuracy: 0.6972 - 1s/epoch - 7ms/step\n",
      "Epoch 124/256\n",
      "149/149 - 1s - loss: 0.0684 - accuracy: 0.9774 - val_loss: 1.8144 - val_accuracy: 0.6998 - 1s/epoch - 7ms/step\n",
      "Epoch 125/256\n",
      "149/149 - 1s - loss: 0.0630 - accuracy: 0.9781 - val_loss: 1.7905 - val_accuracy: 0.7040 - 1s/epoch - 7ms/step\n",
      "Epoch 126/256\n",
      "149/149 - 1s - loss: 0.0631 - accuracy: 0.9791 - val_loss: 1.7958 - val_accuracy: 0.7000 - 1s/epoch - 7ms/step\n",
      "Epoch 127/256\n",
      "149/149 - 1s - loss: 0.0675 - accuracy: 0.9769 - val_loss: 1.8155 - val_accuracy: 0.7014 - 1s/epoch - 7ms/step\n",
      "Epoch 128/256\n",
      "149/149 - 1s - loss: 0.0713 - accuracy: 0.9759 - val_loss: 1.8789 - val_accuracy: 0.6932 - 1s/epoch - 7ms/step\n",
      "Epoch 129/256\n",
      "149/149 - 1s - loss: 0.0663 - accuracy: 0.9781 - val_loss: 1.8447 - val_accuracy: 0.6912 - 1s/epoch - 7ms/step\n",
      "Epoch 130/256\n",
      "149/149 - 1s - loss: 0.0722 - accuracy: 0.9758 - val_loss: 1.8527 - val_accuracy: 0.6944 - 1s/epoch - 7ms/step\n",
      "Epoch 131/256\n",
      "149/149 - 1s - loss: 0.0700 - accuracy: 0.9764 - val_loss: 1.8641 - val_accuracy: 0.6994 - 1s/epoch - 7ms/step\n",
      "Epoch 132/256\n",
      "149/149 - 1s - loss: 0.0626 - accuracy: 0.9781 - val_loss: 1.8626 - val_accuracy: 0.6910 - 1s/epoch - 7ms/step\n",
      "Epoch 133/256\n",
      "149/149 - 1s - loss: 0.0705 - accuracy: 0.9778 - val_loss: 1.7658 - val_accuracy: 0.6898 - 1s/epoch - 7ms/step\n",
      "Epoch 134/256\n",
      "149/149 - 1s - loss: 0.0672 - accuracy: 0.9764 - val_loss: 1.8061 - val_accuracy: 0.6912 - 1s/epoch - 7ms/step\n",
      "Epoch 135/256\n",
      "149/149 - 1s - loss: 0.0650 - accuracy: 0.9784 - val_loss: 1.8389 - val_accuracy: 0.6888 - 1s/epoch - 7ms/step\n",
      "Epoch 136/256\n",
      "149/149 - 1s - loss: 0.0589 - accuracy: 0.9797 - val_loss: 1.8135 - val_accuracy: 0.6960 - 1s/epoch - 7ms/step\n",
      "Epoch 137/256\n",
      "149/149 - 1s - loss: 0.0637 - accuracy: 0.9792 - val_loss: 1.9027 - val_accuracy: 0.6894 - 1s/epoch - 7ms/step\n",
      "Epoch 138/256\n",
      "149/149 - 1s - loss: 0.0580 - accuracy: 0.9807 - val_loss: 1.8839 - val_accuracy: 0.6938 - 1s/epoch - 7ms/step\n",
      "Epoch 139/256\n",
      "149/149 - 1s - loss: 0.0595 - accuracy: 0.9797 - val_loss: 1.8220 - val_accuracy: 0.7006 - 1s/epoch - 7ms/step\n",
      "Epoch 140/256\n",
      "149/149 - 1s - loss: 0.0609 - accuracy: 0.9806 - val_loss: 1.7893 - val_accuracy: 0.6954 - 1s/epoch - 7ms/step\n",
      "Epoch 141/256\n",
      "149/149 - 1s - loss: 0.0579 - accuracy: 0.9799 - val_loss: 1.8862 - val_accuracy: 0.6952 - 1s/epoch - 7ms/step\n",
      "Epoch 142/256\n",
      "149/149 - 1s - loss: 0.0594 - accuracy: 0.9814 - val_loss: 1.8800 - val_accuracy: 0.6942 - 1s/epoch - 7ms/step\n",
      "Epoch 143/256\n",
      "149/149 - 1s - loss: 0.0543 - accuracy: 0.9821 - val_loss: 1.8542 - val_accuracy: 0.6978 - 1s/epoch - 7ms/step\n",
      "Epoch 144/256\n",
      "149/149 - 1s - loss: 0.0624 - accuracy: 0.9783 - val_loss: 1.8565 - val_accuracy: 0.6990 - 1s/epoch - 7ms/step\n",
      "Epoch 145/256\n",
      "149/149 - 1s - loss: 0.0651 - accuracy: 0.9778 - val_loss: 1.8516 - val_accuracy: 0.6932 - 1s/epoch - 7ms/step\n",
      "Epoch 146/256\n",
      "149/149 - 1s - loss: 0.0566 - accuracy: 0.9789 - val_loss: 1.9369 - val_accuracy: 0.7028 - 1s/epoch - 7ms/step\n",
      "Epoch 147/256\n",
      "149/149 - 1s - loss: 0.0671 - accuracy: 0.9776 - val_loss: 1.9060 - val_accuracy: 0.6916 - 1s/epoch - 7ms/step\n",
      "Epoch 148/256\n",
      "149/149 - 1s - loss: 0.0674 - accuracy: 0.9774 - val_loss: 1.8780 - val_accuracy: 0.6930 - 1s/epoch - 7ms/step\n",
      "Epoch 149/256\n",
      "149/149 - 1s - loss: 0.0608 - accuracy: 0.9788 - val_loss: 1.8513 - val_accuracy: 0.6978 - 1s/epoch - 7ms/step\n",
      "Epoch 150/256\n",
      "149/149 - 1s - loss: 0.0693 - accuracy: 0.9773 - val_loss: 1.8412 - val_accuracy: 0.6892 - 1s/epoch - 7ms/step\n",
      "Epoch 151/256\n",
      "149/149 - 1s - loss: 0.0531 - accuracy: 0.9828 - val_loss: 1.9105 - val_accuracy: 0.6972 - 1s/epoch - 8ms/step\n",
      "Epoch 152/256\n",
      "149/149 - 1s - loss: 0.0599 - accuracy: 0.9795 - val_loss: 1.9503 - val_accuracy: 0.6940 - 1s/epoch - 7ms/step\n",
      "Epoch 153/256\n",
      "149/149 - 1s - loss: 0.0560 - accuracy: 0.9808 - val_loss: 1.8658 - val_accuracy: 0.6962 - 1s/epoch - 7ms/step\n",
      "Epoch 154/256\n",
      "149/149 - 1s - loss: 0.0611 - accuracy: 0.9797 - val_loss: 1.8859 - val_accuracy: 0.6972 - 1s/epoch - 7ms/step\n",
      "Epoch 155/256\n",
      "149/149 - 1s - loss: 0.0567 - accuracy: 0.9813 - val_loss: 1.8807 - val_accuracy: 0.7004 - 1s/epoch - 7ms/step\n",
      "Epoch 156/256\n",
      "149/149 - 1s - loss: 0.0552 - accuracy: 0.9823 - val_loss: 1.9470 - val_accuracy: 0.6946 - 1s/epoch - 7ms/step\n",
      "Epoch 157/256\n",
      "149/149 - 1s - loss: 0.0630 - accuracy: 0.9794 - val_loss: 1.9251 - val_accuracy: 0.7004 - 1s/epoch - 7ms/step\n",
      "Epoch 158/256\n",
      "149/149 - 1s - loss: 0.0614 - accuracy: 0.9804 - val_loss: 1.9492 - val_accuracy: 0.6950 - 1s/epoch - 7ms/step\n",
      "Epoch 159/256\n",
      "149/149 - 1s - loss: 0.0591 - accuracy: 0.9798 - val_loss: 1.9327 - val_accuracy: 0.6930 - 1s/epoch - 7ms/step\n",
      "Epoch 160/256\n",
      "149/149 - 1s - loss: 0.0573 - accuracy: 0.9815 - val_loss: 1.8821 - val_accuracy: 0.6992 - 1s/epoch - 7ms/step\n",
      "Epoch 161/256\n",
      "149/149 - 1s - loss: 0.0654 - accuracy: 0.9797 - val_loss: 1.8771 - val_accuracy: 0.6952 - 1s/epoch - 7ms/step\n",
      "Epoch 162/256\n",
      "149/149 - 1s - loss: 0.0584 - accuracy: 0.9807 - val_loss: 1.9380 - val_accuracy: 0.6908 - 1s/epoch - 7ms/step\n",
      "Epoch 163/256\n",
      "149/149 - 1s - loss: 0.0525 - accuracy: 0.9824 - val_loss: 1.9264 - val_accuracy: 0.6964 - 1s/epoch - 7ms/step\n",
      "Epoch 164/256\n",
      "149/149 - 1s - loss: 0.0501 - accuracy: 0.9837 - val_loss: 1.9556 - val_accuracy: 0.6962 - 1s/epoch - 7ms/step\n",
      "Epoch 165/256\n",
      "149/149 - 1s - loss: 0.0527 - accuracy: 0.9831 - val_loss: 1.9460 - val_accuracy: 0.6982 - 1s/epoch - 7ms/step\n",
      "Epoch 166/256\n",
      "149/149 - 1s - loss: 0.0521 - accuracy: 0.9831 - val_loss: 1.9517 - val_accuracy: 0.7014 - 1s/epoch - 8ms/step\n",
      "Epoch 167/256\n",
      "149/149 - 1s - loss: 0.0558 - accuracy: 0.9823 - val_loss: 1.9416 - val_accuracy: 0.6932 - 1s/epoch - 7ms/step\n",
      "Epoch 168/256\n",
      "149/149 - 1s - loss: 0.0523 - accuracy: 0.9829 - val_loss: 1.9376 - val_accuracy: 0.7000 - 1s/epoch - 7ms/step\n",
      "Epoch 169/256\n",
      "149/149 - 1s - loss: 0.0569 - accuracy: 0.9816 - val_loss: 1.9835 - val_accuracy: 0.6986 - 1s/epoch - 7ms/step\n",
      "Epoch 170/256\n",
      "149/149 - 1s - loss: 0.0560 - accuracy: 0.9816 - val_loss: 1.9015 - val_accuracy: 0.6980 - 1s/epoch - 7ms/step\n",
      "Epoch 171/256\n",
      "149/149 - 1s - loss: 0.0465 - accuracy: 0.9853 - val_loss: 2.0282 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 172/256\n",
      "149/149 - 1s - loss: 0.0548 - accuracy: 0.9819 - val_loss: 2.0401 - val_accuracy: 0.6964 - 1s/epoch - 7ms/step\n",
      "Epoch 173/256\n",
      "149/149 - 1s - loss: 0.0562 - accuracy: 0.9818 - val_loss: 2.1103 - val_accuracy: 0.6848 - 1s/epoch - 7ms/step\n",
      "Epoch 174/256\n",
      "149/149 - 1s - loss: 0.0594 - accuracy: 0.9810 - val_loss: 1.9717 - val_accuracy: 0.6928 - 1s/epoch - 7ms/step\n",
      "Epoch 175/256\n",
      "149/149 - 1s - loss: 0.0502 - accuracy: 0.9834 - val_loss: 2.0529 - val_accuracy: 0.6988 - 1s/epoch - 7ms/step\n",
      "Epoch 176/256\n",
      "149/149 - 1s - loss: 0.0542 - accuracy: 0.9826 - val_loss: 2.0278 - val_accuracy: 0.6888 - 1s/epoch - 8ms/step\n",
      "Epoch 177/256\n",
      "149/149 - 1s - loss: 0.0541 - accuracy: 0.9814 - val_loss: 1.9911 - val_accuracy: 0.7010 - 1s/epoch - 7ms/step\n",
      "Epoch 178/256\n",
      "149/149 - 1s - loss: 0.0572 - accuracy: 0.9808 - val_loss: 2.0255 - val_accuracy: 0.7018 - 1s/epoch - 7ms/step\n",
      "Epoch 179/256\n",
      "149/149 - 1s - loss: 0.0554 - accuracy: 0.9831 - val_loss: 1.9302 - val_accuracy: 0.6986 - 1s/epoch - 7ms/step\n",
      "Epoch 180/256\n",
      "149/149 - 1s - loss: 0.0548 - accuracy: 0.9812 - val_loss: 2.0062 - val_accuracy: 0.6930 - 1s/epoch - 7ms/step\n",
      "Epoch 181/256\n",
      "149/149 - 1s - loss: 0.0599 - accuracy: 0.9806 - val_loss: 1.9598 - val_accuracy: 0.6952 - 1s/epoch - 7ms/step\n",
      "Epoch 182/256\n",
      "149/149 - 1s - loss: 0.0531 - accuracy: 0.9810 - val_loss: 1.9865 - val_accuracy: 0.6932 - 1s/epoch - 7ms/step\n",
      "Epoch 183/256\n",
      "149/149 - 1s - loss: 0.0494 - accuracy: 0.9841 - val_loss: 2.0000 - val_accuracy: 0.6982 - 1s/epoch - 7ms/step\n",
      "Epoch 184/256\n",
      "149/149 - 1s - loss: 0.0524 - accuracy: 0.9834 - val_loss: 1.9618 - val_accuracy: 0.6950 - 1s/epoch - 7ms/step\n",
      "Epoch 185/256\n",
      "149/149 - 1s - loss: 0.0552 - accuracy: 0.9819 - val_loss: 1.9287 - val_accuracy: 0.7030 - 1s/epoch - 7ms/step\n",
      "Epoch 186/256\n",
      "149/149 - 1s - loss: 0.0584 - accuracy: 0.9809 - val_loss: 1.9904 - val_accuracy: 0.6974 - 1s/epoch - 7ms/step\n",
      "Epoch 187/256\n",
      "149/149 - 1s - loss: 0.0527 - accuracy: 0.9831 - val_loss: 1.9980 - val_accuracy: 0.6874 - 1s/epoch - 7ms/step\n",
      "Epoch 188/256\n",
      "149/149 - 1s - loss: 0.0583 - accuracy: 0.9807 - val_loss: 2.0173 - val_accuracy: 0.6960 - 1s/epoch - 7ms/step\n",
      "Epoch 189/256\n",
      "149/149 - 1s - loss: 0.0494 - accuracy: 0.9826 - val_loss: 1.9241 - val_accuracy: 0.6980 - 1s/epoch - 7ms/step\n",
      "Epoch 190/256\n",
      "149/149 - 1s - loss: 0.0517 - accuracy: 0.9825 - val_loss: 2.0048 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 191/256\n",
      "149/149 - 1s - loss: 0.0534 - accuracy: 0.9825 - val_loss: 2.0791 - val_accuracy: 0.6894 - 1s/epoch - 7ms/step\n",
      "Epoch 192/256\n",
      "149/149 - 1s - loss: 0.0515 - accuracy: 0.9830 - val_loss: 2.1032 - val_accuracy: 0.6954 - 1s/epoch - 7ms/step\n",
      "Epoch 193/256\n",
      "149/149 - 1s - loss: 0.0579 - accuracy: 0.9823 - val_loss: 1.9574 - val_accuracy: 0.6954 - 1s/epoch - 7ms/step\n",
      "Epoch 194/256\n",
      "149/149 - 1s - loss: 0.0474 - accuracy: 0.9832 - val_loss: 1.9505 - val_accuracy: 0.6958 - 1s/epoch - 7ms/step\n",
      "Epoch 195/256\n",
      "149/149 - 1s - loss: 0.0513 - accuracy: 0.9829 - val_loss: 2.0292 - val_accuracy: 0.6942 - 1s/epoch - 7ms/step\n",
      "Epoch 196/256\n",
      "149/149 - 1s - loss: 0.0513 - accuracy: 0.9823 - val_loss: 2.0381 - val_accuracy: 0.6938 - 1s/epoch - 7ms/step\n",
      "Epoch 197/256\n",
      "149/149 - 1s - loss: 0.0530 - accuracy: 0.9826 - val_loss: 2.0296 - val_accuracy: 0.6956 - 1s/epoch - 7ms/step\n",
      "Epoch 198/256\n",
      "149/149 - 1s - loss: 0.0532 - accuracy: 0.9816 - val_loss: 2.0534 - val_accuracy: 0.6930 - 1s/epoch - 7ms/step\n",
      "Epoch 199/256\n",
      "149/149 - 1s - loss: 0.0486 - accuracy: 0.9847 - val_loss: 1.9712 - val_accuracy: 0.6916 - 1s/epoch - 7ms/step\n",
      "Epoch 200/256\n",
      "149/149 - 1s - loss: 0.0433 - accuracy: 0.9856 - val_loss: 2.0420 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 201/256\n",
      "149/149 - 1s - loss: 0.0485 - accuracy: 0.9841 - val_loss: 2.0289 - val_accuracy: 0.6956 - 1s/epoch - 8ms/step\n",
      "Epoch 202/256\n",
      "149/149 - 1s - loss: 0.0533 - accuracy: 0.9830 - val_loss: 2.0573 - val_accuracy: 0.6992 - 1s/epoch - 7ms/step\n",
      "Epoch 203/256\n",
      "149/149 - 1s - loss: 0.0469 - accuracy: 0.9845 - val_loss: 2.0753 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 204/256\n",
      "149/149 - 1s - loss: 0.0481 - accuracy: 0.9840 - val_loss: 2.0692 - val_accuracy: 0.6992 - 1s/epoch - 7ms/step\n",
      "Epoch 205/256\n",
      "149/149 - 1s - loss: 0.0548 - accuracy: 0.9824 - val_loss: 2.0838 - val_accuracy: 0.6928 - 1s/epoch - 7ms/step\n",
      "Epoch 206/256\n",
      "149/149 - 1s - loss: 0.0512 - accuracy: 0.9841 - val_loss: 2.0456 - val_accuracy: 0.7024 - 1s/epoch - 8ms/step\n",
      "Epoch 207/256\n",
      "149/149 - 1s - loss: 0.0453 - accuracy: 0.9852 - val_loss: 2.1615 - val_accuracy: 0.6958 - 1s/epoch - 7ms/step\n",
      "Epoch 208/256\n",
      "149/149 - 1s - loss: 0.0514 - accuracy: 0.9836 - val_loss: 2.0855 - val_accuracy: 0.6972 - 1s/epoch - 9ms/step\n",
      "Epoch 209/256\n",
      "149/149 - 1s - loss: 0.0482 - accuracy: 0.9846 - val_loss: 2.0869 - val_accuracy: 0.7022 - 1s/epoch - 9ms/step\n",
      "Epoch 210/256\n",
      "149/149 - 1s - loss: 0.0405 - accuracy: 0.9861 - val_loss: 2.1307 - val_accuracy: 0.7046 - 1s/epoch - 8ms/step\n",
      "Epoch 211/256\n",
      "149/149 - 1s - loss: 0.0519 - accuracy: 0.9832 - val_loss: 2.0776 - val_accuracy: 0.6962 - 1s/epoch - 7ms/step\n",
      "Epoch 212/256\n",
      "149/149 - 1s - loss: 0.0465 - accuracy: 0.9844 - val_loss: 2.1708 - val_accuracy: 0.7030 - 1s/epoch - 7ms/step\n",
      "Epoch 213/256\n",
      "149/149 - 1s - loss: 0.0550 - accuracy: 0.9810 - val_loss: 2.0824 - val_accuracy: 0.6958 - 1s/epoch - 7ms/step\n",
      "Epoch 214/256\n",
      "149/149 - 1s - loss: 0.0484 - accuracy: 0.9839 - val_loss: 2.1749 - val_accuracy: 0.6948 - 1s/epoch - 7ms/step\n",
      "Epoch 215/256\n",
      "149/149 - 1s - loss: 0.0468 - accuracy: 0.9852 - val_loss: 2.1105 - val_accuracy: 0.6902 - 1s/epoch - 7ms/step\n",
      "Epoch 216/256\n",
      "149/149 - 1s - loss: 0.0455 - accuracy: 0.9844 - val_loss: 2.2535 - val_accuracy: 0.6870 - 1s/epoch - 7ms/step\n",
      "Epoch 217/256\n",
      "149/149 - 1s - loss: 0.0449 - accuracy: 0.9856 - val_loss: 2.1816 - val_accuracy: 0.6870 - 1s/epoch - 7ms/step\n",
      "Epoch 218/256\n",
      "149/149 - 1s - loss: 0.0532 - accuracy: 0.9825 - val_loss: 2.0619 - val_accuracy: 0.6958 - 1s/epoch - 7ms/step\n",
      "Epoch 219/256\n",
      "149/149 - 1s - loss: 0.0421 - accuracy: 0.9872 - val_loss: 2.1436 - val_accuracy: 0.6950 - 1s/epoch - 7ms/step\n",
      "Epoch 220/256\n",
      "149/149 - 1s - loss: 0.0432 - accuracy: 0.9859 - val_loss: 2.1148 - val_accuracy: 0.6976 - 1s/epoch - 7ms/step\n",
      "Epoch 221/256\n",
      "149/149 - 1s - loss: 0.0424 - accuracy: 0.9857 - val_loss: 2.0814 - val_accuracy: 0.6970 - 1s/epoch - 7ms/step\n",
      "Epoch 222/256\n",
      "149/149 - 1s - loss: 0.0498 - accuracy: 0.9840 - val_loss: 2.1667 - val_accuracy: 0.6946 - 1s/epoch - 7ms/step\n",
      "Epoch 223/256\n",
      "149/149 - 1s - loss: 0.0456 - accuracy: 0.9855 - val_loss: 2.1260 - val_accuracy: 0.7046 - 1s/epoch - 7ms/step\n",
      "Epoch 224/256\n",
      "149/149 - 1s - loss: 0.0517 - accuracy: 0.9828 - val_loss: 2.1199 - val_accuracy: 0.6994 - 1s/epoch - 7ms/step\n",
      "Epoch 225/256\n",
      "149/149 - 1s - loss: 0.0495 - accuracy: 0.9837 - val_loss: 2.0600 - val_accuracy: 0.6912 - 1s/epoch - 7ms/step\n",
      "Epoch 226/256\n",
      "149/149 - 1s - loss: 0.0525 - accuracy: 0.9825 - val_loss: 2.0998 - val_accuracy: 0.6976 - 1s/epoch - 7ms/step\n",
      "Epoch 227/256\n",
      "149/149 - 1s - loss: 0.0452 - accuracy: 0.9861 - val_loss: 2.0676 - val_accuracy: 0.6954 - 1s/epoch - 7ms/step\n",
      "Epoch 228/256\n",
      "149/149 - 1s - loss: 0.0432 - accuracy: 0.9856 - val_loss: 2.1069 - val_accuracy: 0.6926 - 1s/epoch - 7ms/step\n",
      "Epoch 229/256\n",
      "149/149 - 1s - loss: 0.0446 - accuracy: 0.9856 - val_loss: 2.1376 - val_accuracy: 0.6946 - 1s/epoch - 7ms/step\n",
      "Epoch 230/256\n",
      "149/149 - 1s - loss: 0.0440 - accuracy: 0.9849 - val_loss: 2.1036 - val_accuracy: 0.6958 - 1s/epoch - 7ms/step\n",
      "Epoch 231/256\n",
      "149/149 - 1s - loss: 0.0442 - accuracy: 0.9851 - val_loss: 2.0930 - val_accuracy: 0.6954 - 1s/epoch - 7ms/step\n",
      "Epoch 232/256\n",
      "149/149 - 1s - loss: 0.0509 - accuracy: 0.9832 - val_loss: 2.1192 - val_accuracy: 0.6940 - 1s/epoch - 7ms/step\n",
      "Epoch 233/256\n",
      "149/149 - 1s - loss: 0.0446 - accuracy: 0.9833 - val_loss: 2.2307 - val_accuracy: 0.6988 - 1s/epoch - 7ms/step\n",
      "Epoch 234/256\n",
      "149/149 - 1s - loss: 0.0402 - accuracy: 0.9873 - val_loss: 2.1241 - val_accuracy: 0.6942 - 1s/epoch - 7ms/step\n",
      "Epoch 235/256\n",
      "149/149 - 1s - loss: 0.0512 - accuracy: 0.9837 - val_loss: 2.1363 - val_accuracy: 0.6910 - 1s/epoch - 7ms/step\n",
      "Epoch 236/256\n",
      "149/149 - 1s - loss: 0.0447 - accuracy: 0.9849 - val_loss: 2.1983 - val_accuracy: 0.6982 - 1s/epoch - 7ms/step\n",
      "Epoch 237/256\n",
      "149/149 - 1s - loss: 0.0483 - accuracy: 0.9841 - val_loss: 2.2310 - val_accuracy: 0.6980 - 1s/epoch - 7ms/step\n",
      "Epoch 238/256\n",
      "149/149 - 1s - loss: 0.0435 - accuracy: 0.9865 - val_loss: 2.2152 - val_accuracy: 0.6932 - 1s/epoch - 7ms/step\n",
      "Epoch 239/256\n",
      "149/149 - 1s - loss: 0.0472 - accuracy: 0.9847 - val_loss: 2.2825 - val_accuracy: 0.6936 - 1s/epoch - 7ms/step\n",
      "Epoch 240/256\n",
      "149/149 - 1s - loss: 0.0513 - accuracy: 0.9846 - val_loss: 2.2648 - val_accuracy: 0.6954 - 1s/epoch - 7ms/step\n",
      "Epoch 241/256\n",
      "149/149 - 1s - loss: 0.0459 - accuracy: 0.9849 - val_loss: 2.2044 - val_accuracy: 0.6984 - 1s/epoch - 7ms/step\n",
      "Epoch 242/256\n",
      "149/149 - 1s - loss: 0.0526 - accuracy: 0.9851 - val_loss: 2.1580 - val_accuracy: 0.6986 - 1s/epoch - 7ms/step\n",
      "Epoch 243/256\n",
      "149/149 - 1s - loss: 0.0470 - accuracy: 0.9847 - val_loss: 2.0480 - val_accuracy: 0.7008 - 1s/epoch - 7ms/step\n",
      "Epoch 244/256\n",
      "149/149 - 1s - loss: 0.0444 - accuracy: 0.9857 - val_loss: 2.1293 - val_accuracy: 0.6968 - 1s/epoch - 7ms/step\n",
      "Epoch 245/256\n",
      "149/149 - 1s - loss: 0.0454 - accuracy: 0.9856 - val_loss: 2.1151 - val_accuracy: 0.6920 - 1s/epoch - 7ms/step\n",
      "Epoch 246/256\n",
      "149/149 - 1s - loss: 0.0436 - accuracy: 0.9857 - val_loss: 2.1343 - val_accuracy: 0.6988 - 1s/epoch - 7ms/step\n",
      "Epoch 247/256\n",
      "149/149 - 1s - loss: 0.0448 - accuracy: 0.9861 - val_loss: 2.1070 - val_accuracy: 0.7034 - 1s/epoch - 7ms/step\n",
      "Epoch 248/256\n",
      "149/149 - 1s - loss: 0.0409 - accuracy: 0.9864 - val_loss: 2.0960 - val_accuracy: 0.7024 - 1s/epoch - 7ms/step\n",
      "Epoch 249/256\n",
      "149/149 - 1s - loss: 0.0489 - accuracy: 0.9851 - val_loss: 2.1615 - val_accuracy: 0.6948 - 1s/epoch - 7ms/step\n",
      "Epoch 250/256\n",
      "149/149 - 1s - loss: 0.0409 - accuracy: 0.9867 - val_loss: 2.1567 - val_accuracy: 0.7028 - 1s/epoch - 7ms/step\n",
      "Epoch 251/256\n",
      "149/149 - 1s - loss: 0.0466 - accuracy: 0.9854 - val_loss: 2.1485 - val_accuracy: 0.7010 - 1s/epoch - 7ms/step\n",
      "Epoch 252/256\n",
      "149/149 - 1s - loss: 0.0407 - accuracy: 0.9871 - val_loss: 2.2846 - val_accuracy: 0.6938 - 1s/epoch - 7ms/step\n",
      "Epoch 253/256\n",
      "149/149 - 1s - loss: 0.0448 - accuracy: 0.9859 - val_loss: 2.2504 - val_accuracy: 0.6994 - 1s/epoch - 7ms/step\n",
      "Epoch 254/256\n",
      "149/149 - 1s - loss: 0.0444 - accuracy: 0.9850 - val_loss: 2.1402 - val_accuracy: 0.6982 - 1s/epoch - 7ms/step\n",
      "Epoch 255/256\n",
      "149/149 - 1s - loss: 0.0410 - accuracy: 0.9874 - val_loss: 2.2231 - val_accuracy: 0.6958 - 1s/epoch - 7ms/step\n",
      "Epoch 256/256\n",
      "149/149 - 1s - loss: 0.0452 - accuracy: 0.9861 - val_loss: 2.1385 - val_accuracy: 0.6964 - 1s/epoch - 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47f07038d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=Adam(lr=0.001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
    "\n",
    "model.fit( x_train[0:19000], y_train[0:19000],\n",
    "          batch_size=128, epochs=256, verbose=2,\n",
    "          validation_data=(x_test[0:5000], y_test[0:5000]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4UpAjaexR8O",
    "outputId": "1c3c9f5f-f282-4d16-e2d7-fc4d23fb9039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step\n",
      "Accuracy Score : 0.6964\n",
      "Averaged Precision Score : 0.6945907378863102\n",
      "Averaged Recall Score : 0.6964\n",
      "Averaged F1 Score : 0.6936421497531267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.67      0.69       488\n",
      "           1       0.81      0.86      0.83       505\n",
      "           2       0.60      0.56      0.58       512\n",
      "           3       0.53      0.49      0.51       497\n",
      "           4       0.61      0.72      0.66       507\n",
      "           5       0.62      0.52      0.57       488\n",
      "           6       0.74      0.77      0.76       491\n",
      "           7       0.77      0.76      0.77       495\n",
      "           8       0.74      0.87      0.80       504\n",
      "           9       0.81      0.75      0.78       513\n",
      "\n",
      "    accuracy                           0.70      5000\n",
      "   macro avg       0.69      0.70      0.69      5000\n",
      "weighted avg       0.69      0.70      0.69      5000\n",
      "\n",
      "[2.1384527683258057, 0.696399986743927]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = np.argmax(y_test[0:5000], axis=1)\n",
    "prediction = model.predict(x_test[0:5000])\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "\n",
    "metrics = { 'Accuracy Score' : metrics.accuracy_score(y_true, prediction),\n",
    "            'Averaged Precision Score' : metrics.precision_score(y_true, prediction, average='weighted'),\n",
    "            'Averaged Recall Score' : metrics.recall_score(y_true, prediction, average='weighted'),\n",
    "            'Averaged F1 Score' : metrics.f1_score(y_true, prediction, average='weighted')}\n",
    "for k,v in metrics.items():\n",
    "  print(f'{k} : {v}')\n",
    "\n",
    "print(classification_report(y_true, prediction))\n",
    "print( model.evaluate(x_test[0:5000], y_test[0:5000], verbose=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91gzVR6wxQys",
    "outputId": "30a789fc-0bd3-4588-e8c0-ea83fd9b96a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photograph #1794... Actual Classification: 3 -> Predicted Classification: 3\n",
      "Photograph #3305... Actual Classification: 9 -> Predicted Classification: 9\n",
      "Photograph #912... Actual Classification: 0 -> Predicted Classification: 2\n",
      "Photograph #2507... Actual Classification: 0 -> Predicted Classification: 0\n",
      "Photograph #1740... Actual Classification: 1 -> Predicted Classification: 1\n",
      "Photograph #1372... Actual Classification: 1 -> Predicted Classification: 1\n",
      "Photograph #3904... Actual Classification: 1 -> Predicted Classification: 1\n",
      "Photograph #68... Actual Classification: 3 -> Predicted Classification: 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 8):\n",
    "  idx = np.random.randint(1,prediction.shape[0])\n",
    "  print(f'Photograph #{idx}... Actual Classification: {y_true[idx]} -> Predicted Classification: {prediction[idx]}')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tA4dAf2vN8t"
   },
   "source": [
    "Test Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9oZJeB9d7o2f",
    "outputId": "40303b6b-8190-41ac-a752-f03595e32847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_101 (Conv2D)         (None, 14, 14, 32)        3488      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_90 (MaxPoolin  (None, 7, 7, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_65 (Dropout)        (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " conv2d_102 (Conv2D)         (None, 7, 7, 48)          13872     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 7, 7, 48)          0         \n",
      "                                                                 \n",
      " max_pooling2d_91 (MaxPoolin  (None, 3, 3, 48)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 3, 3, 48)          0         \n",
      "                                                                 \n",
      " conv2d_103 (Conv2D)         (None, 3, 3, 64)          27712     \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 3, 3, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_92 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_67 (Dropout)        (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " flatten_33 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 2048)              133120    \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 2048)              0         \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,682\n",
      "Trainable params: 198,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(6, 6), strides=(2, 2),\n",
    "                 padding='valid', input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Conv2D(48, (3, 3), strides=(1, 1),\n",
    "                 padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1),\n",
    "                 padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.70))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljXMnoZw7vcC",
    "outputId": "b3466266-24b0-457b-8910-dd8a5d787b23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "391/391 - 3s - loss: 1.8638 - accuracy: 0.2939 - val_loss: 1.5282 - val_accuracy: 0.4331 - 3s/epoch - 7ms/step\n",
      "Epoch 2/256\n",
      "391/391 - 2s - loss: 1.5371 - accuracy: 0.4285 - val_loss: 1.4509 - val_accuracy: 0.4776 - 2s/epoch - 5ms/step\n",
      "Epoch 3/256\n",
      "391/391 - 2s - loss: 1.4330 - accuracy: 0.4759 - val_loss: 1.2617 - val_accuracy: 0.5446 - 2s/epoch - 4ms/step\n",
      "Epoch 4/256\n",
      "391/391 - 2s - loss: 1.3533 - accuracy: 0.5075 - val_loss: 1.2211 - val_accuracy: 0.5582 - 2s/epoch - 4ms/step\n",
      "Epoch 5/256\n",
      "391/391 - 2s - loss: 1.2996 - accuracy: 0.5309 - val_loss: 1.2006 - val_accuracy: 0.5749 - 2s/epoch - 5ms/step\n",
      "Epoch 6/256\n",
      "391/391 - 2s - loss: 1.2562 - accuracy: 0.5483 - val_loss: 1.1304 - val_accuracy: 0.5981 - 2s/epoch - 5ms/step\n",
      "Epoch 7/256\n",
      "391/391 - 2s - loss: 1.2191 - accuracy: 0.5632 - val_loss: 1.0890 - val_accuracy: 0.6089 - 2s/epoch - 4ms/step\n",
      "Epoch 8/256\n",
      "391/391 - 2s - loss: 1.1811 - accuracy: 0.5762 - val_loss: 1.0706 - val_accuracy: 0.6133 - 2s/epoch - 4ms/step\n",
      "Epoch 9/256\n",
      "391/391 - 2s - loss: 1.1540 - accuracy: 0.5878 - val_loss: 1.0656 - val_accuracy: 0.6144 - 2s/epoch - 5ms/step\n",
      "Epoch 10/256\n",
      "391/391 - 2s - loss: 1.1311 - accuracy: 0.5973 - val_loss: 1.0652 - val_accuracy: 0.6230 - 2s/epoch - 5ms/step\n",
      "Epoch 11/256\n",
      "391/391 - 2s - loss: 1.1090 - accuracy: 0.6036 - val_loss: 1.0239 - val_accuracy: 0.6375 - 2s/epoch - 5ms/step\n",
      "Epoch 12/256\n",
      "391/391 - 2s - loss: 1.0973 - accuracy: 0.6106 - val_loss: 0.9916 - val_accuracy: 0.6506 - 2s/epoch - 5ms/step\n",
      "Epoch 13/256\n",
      "391/391 - 2s - loss: 1.0759 - accuracy: 0.6186 - val_loss: 0.9863 - val_accuracy: 0.6525 - 2s/epoch - 5ms/step\n",
      "Epoch 14/256\n",
      "391/391 - 2s - loss: 1.0672 - accuracy: 0.6239 - val_loss: 0.9787 - val_accuracy: 0.6567 - 2s/epoch - 5ms/step\n",
      "Epoch 15/256\n",
      "391/391 - 2s - loss: 1.0479 - accuracy: 0.6269 - val_loss: 0.9696 - val_accuracy: 0.6604 - 2s/epoch - 5ms/step\n",
      "Epoch 16/256\n",
      "391/391 - 2s - loss: 1.0401 - accuracy: 0.6356 - val_loss: 0.9527 - val_accuracy: 0.6612 - 2s/epoch - 5ms/step\n",
      "Epoch 17/256\n",
      "391/391 - 2s - loss: 1.0320 - accuracy: 0.6343 - val_loss: 0.9503 - val_accuracy: 0.6675 - 2s/epoch - 5ms/step\n",
      "Epoch 18/256\n",
      "391/391 - 2s - loss: 1.0101 - accuracy: 0.6410 - val_loss: 0.9345 - val_accuracy: 0.6739 - 2s/epoch - 5ms/step\n",
      "Epoch 19/256\n",
      "391/391 - 2s - loss: 1.0062 - accuracy: 0.6474 - val_loss: 0.9202 - val_accuracy: 0.6795 - 2s/epoch - 5ms/step\n",
      "Epoch 20/256\n",
      "391/391 - 3s - loss: 1.0002 - accuracy: 0.6458 - val_loss: 0.9166 - val_accuracy: 0.6807 - 3s/epoch - 7ms/step\n",
      "Epoch 21/256\n",
      "391/391 - 2s - loss: 0.9911 - accuracy: 0.6554 - val_loss: 0.9178 - val_accuracy: 0.6787 - 2s/epoch - 5ms/step\n",
      "Epoch 22/256\n",
      "391/391 - 2s - loss: 0.9841 - accuracy: 0.6519 - val_loss: 0.9416 - val_accuracy: 0.6690 - 2s/epoch - 5ms/step\n",
      "Epoch 23/256\n",
      "391/391 - 2s - loss: 0.9721 - accuracy: 0.6557 - val_loss: 0.8941 - val_accuracy: 0.6861 - 2s/epoch - 5ms/step\n",
      "Epoch 24/256\n",
      "391/391 - 2s - loss: 0.9713 - accuracy: 0.6553 - val_loss: 0.9044 - val_accuracy: 0.6811 - 2s/epoch - 5ms/step\n",
      "Epoch 25/256\n",
      "391/391 - 2s - loss: 0.9600 - accuracy: 0.6617 - val_loss: 0.9594 - val_accuracy: 0.6648 - 2s/epoch - 6ms/step\n",
      "Epoch 26/256\n",
      "391/391 - 2s - loss: 0.9616 - accuracy: 0.6605 - val_loss: 0.8976 - val_accuracy: 0.6856 - 2s/epoch - 5ms/step\n",
      "Epoch 27/256\n",
      "391/391 - 2s - loss: 0.9573 - accuracy: 0.6624 - val_loss: 0.8866 - val_accuracy: 0.6911 - 2s/epoch - 5ms/step\n",
      "Epoch 28/256\n",
      "391/391 - 2s - loss: 0.9493 - accuracy: 0.6679 - val_loss: 0.9419 - val_accuracy: 0.6764 - 2s/epoch - 5ms/step\n",
      "Epoch 29/256\n",
      "391/391 - 2s - loss: 0.9428 - accuracy: 0.6670 - val_loss: 0.9126 - val_accuracy: 0.6832 - 2s/epoch - 5ms/step\n",
      "Epoch 30/256\n",
      "391/391 - 2s - loss: 0.9363 - accuracy: 0.6670 - val_loss: 0.8741 - val_accuracy: 0.6935 - 2s/epoch - 5ms/step\n",
      "Epoch 31/256\n",
      "391/391 - 3s - loss: 0.9331 - accuracy: 0.6716 - val_loss: 0.8808 - val_accuracy: 0.6924 - 3s/epoch - 6ms/step\n",
      "Epoch 32/256\n",
      "391/391 - 2s - loss: 0.9353 - accuracy: 0.6699 - val_loss: 0.9030 - val_accuracy: 0.6844 - 2s/epoch - 6ms/step\n",
      "Epoch 33/256\n",
      "391/391 - 2s - loss: 0.9271 - accuracy: 0.6750 - val_loss: 0.8749 - val_accuracy: 0.6956 - 2s/epoch - 5ms/step\n",
      "Epoch 34/256\n",
      "391/391 - 2s - loss: 0.9282 - accuracy: 0.6742 - val_loss: 0.8755 - val_accuracy: 0.6918 - 2s/epoch - 5ms/step\n",
      "Epoch 35/256\n",
      "391/391 - 2s - loss: 0.9143 - accuracy: 0.6790 - val_loss: 0.8869 - val_accuracy: 0.6904 - 2s/epoch - 5ms/step\n",
      "Epoch 36/256\n",
      "391/391 - 2s - loss: 0.9219 - accuracy: 0.6747 - val_loss: 0.8664 - val_accuracy: 0.6973 - 2s/epoch - 5ms/step\n",
      "Epoch 37/256\n",
      "391/391 - 2s - loss: 0.9178 - accuracy: 0.6781 - val_loss: 0.8763 - val_accuracy: 0.6970 - 2s/epoch - 5ms/step\n",
      "Epoch 38/256\n",
      "391/391 - 2s - loss: 0.9088 - accuracy: 0.6806 - val_loss: 0.8766 - val_accuracy: 0.7006 - 2s/epoch - 5ms/step\n",
      "Epoch 39/256\n",
      "391/391 - 2s - loss: 0.9138 - accuracy: 0.6809 - val_loss: 0.8969 - val_accuracy: 0.6862 - 2s/epoch - 5ms/step\n",
      "Epoch 40/256\n",
      "391/391 - 2s - loss: 0.9134 - accuracy: 0.6782 - val_loss: 0.8715 - val_accuracy: 0.6930 - 2s/epoch - 5ms/step\n",
      "Epoch 41/256\n",
      "391/391 - 2s - loss: 0.8920 - accuracy: 0.6870 - val_loss: 0.8605 - val_accuracy: 0.7018 - 2s/epoch - 5ms/step\n",
      "Epoch 42/256\n",
      "391/391 - 2s - loss: 0.8990 - accuracy: 0.6845 - val_loss: 0.8611 - val_accuracy: 0.6998 - 2s/epoch - 5ms/step\n",
      "Epoch 43/256\n",
      "391/391 - 2s - loss: 0.8897 - accuracy: 0.6887 - val_loss: 0.8668 - val_accuracy: 0.6968 - 2s/epoch - 5ms/step\n",
      "Epoch 44/256\n",
      "391/391 - 2s - loss: 0.8937 - accuracy: 0.6865 - val_loss: 0.8607 - val_accuracy: 0.6971 - 2s/epoch - 5ms/step\n",
      "Epoch 45/256\n",
      "391/391 - 2s - loss: 0.8939 - accuracy: 0.6862 - val_loss: 0.8739 - val_accuracy: 0.6986 - 2s/epoch - 5ms/step\n",
      "Epoch 46/256\n",
      "391/391 - 2s - loss: 0.8953 - accuracy: 0.6855 - val_loss: 0.8360 - val_accuracy: 0.7127 - 2s/epoch - 5ms/step\n",
      "Epoch 47/256\n",
      "391/391 - 2s - loss: 0.8836 - accuracy: 0.6876 - val_loss: 0.8462 - val_accuracy: 0.7095 - 2s/epoch - 5ms/step\n",
      "Epoch 48/256\n",
      "391/391 - 2s - loss: 0.8856 - accuracy: 0.6871 - val_loss: 0.8509 - val_accuracy: 0.7069 - 2s/epoch - 5ms/step\n",
      "Epoch 49/256\n",
      "391/391 - 2s - loss: 0.8839 - accuracy: 0.6887 - val_loss: 0.8594 - val_accuracy: 0.7028 - 2s/epoch - 5ms/step\n",
      "Epoch 50/256\n",
      "391/391 - 2s - loss: 0.8789 - accuracy: 0.6900 - val_loss: 0.8709 - val_accuracy: 0.7008 - 2s/epoch - 5ms/step\n",
      "Epoch 51/256\n",
      "391/391 - 2s - loss: 0.8759 - accuracy: 0.6956 - val_loss: 0.8380 - val_accuracy: 0.7112 - 2s/epoch - 4ms/step\n",
      "Epoch 52/256\n",
      "391/391 - 2s - loss: 0.8727 - accuracy: 0.6928 - val_loss: 0.8301 - val_accuracy: 0.7118 - 2s/epoch - 5ms/step\n",
      "Epoch 53/256\n",
      "391/391 - 2s - loss: 0.8767 - accuracy: 0.6940 - val_loss: 0.8688 - val_accuracy: 0.6967 - 2s/epoch - 5ms/step\n",
      "Epoch 54/256\n",
      "391/391 - 2s - loss: 0.8731 - accuracy: 0.6958 - val_loss: 0.8566 - val_accuracy: 0.7048 - 2s/epoch - 5ms/step\n",
      "Epoch 55/256\n",
      "391/391 - 2s - loss: 0.8696 - accuracy: 0.6946 - val_loss: 0.8332 - val_accuracy: 0.7155 - 2s/epoch - 5ms/step\n",
      "Epoch 56/256\n",
      "391/391 - 2s - loss: 0.8766 - accuracy: 0.6937 - val_loss: 0.8345 - val_accuracy: 0.7130 - 2s/epoch - 5ms/step\n",
      "Epoch 57/256\n",
      "391/391 - 2s - loss: 0.8661 - accuracy: 0.6978 - val_loss: 0.8291 - val_accuracy: 0.7169 - 2s/epoch - 5ms/step\n",
      "Epoch 58/256\n",
      "391/391 - 2s - loss: 0.8658 - accuracy: 0.6963 - val_loss: 0.8429 - val_accuracy: 0.7136 - 2s/epoch - 5ms/step\n",
      "Epoch 59/256\n",
      "391/391 - 2s - loss: 0.8684 - accuracy: 0.6985 - val_loss: 0.8354 - val_accuracy: 0.7141 - 2s/epoch - 5ms/step\n",
      "Epoch 60/256\n",
      "391/391 - 2s - loss: 0.8662 - accuracy: 0.6958 - val_loss: 0.8263 - val_accuracy: 0.7186 - 2s/epoch - 5ms/step\n",
      "Epoch 61/256\n",
      "391/391 - 2s - loss: 0.8645 - accuracy: 0.6989 - val_loss: 0.8288 - val_accuracy: 0.7158 - 2s/epoch - 5ms/step\n",
      "Epoch 62/256\n",
      "391/391 - 2s - loss: 0.8509 - accuracy: 0.7039 - val_loss: 0.8460 - val_accuracy: 0.7088 - 2s/epoch - 5ms/step\n",
      "Epoch 63/256\n",
      "391/391 - 2s - loss: 0.8614 - accuracy: 0.6985 - val_loss: 0.8456 - val_accuracy: 0.7086 - 2s/epoch - 5ms/step\n",
      "Epoch 64/256\n",
      "391/391 - 2s - loss: 0.8557 - accuracy: 0.7019 - val_loss: 0.8416 - val_accuracy: 0.7113 - 2s/epoch - 5ms/step\n",
      "Epoch 65/256\n",
      "391/391 - 2s - loss: 0.8413 - accuracy: 0.7031 - val_loss: 0.8273 - val_accuracy: 0.7169 - 2s/epoch - 5ms/step\n",
      "Epoch 66/256\n",
      "391/391 - 2s - loss: 0.8527 - accuracy: 0.7011 - val_loss: 0.8479 - val_accuracy: 0.7112 - 2s/epoch - 5ms/step\n",
      "Epoch 67/256\n",
      "391/391 - 2s - loss: 0.8479 - accuracy: 0.7039 - val_loss: 0.8309 - val_accuracy: 0.7145 - 2s/epoch - 5ms/step\n",
      "Epoch 68/256\n",
      "391/391 - 2s - loss: 0.8537 - accuracy: 0.7002 - val_loss: 0.8337 - val_accuracy: 0.7145 - 2s/epoch - 5ms/step\n",
      "Epoch 69/256\n",
      "391/391 - 2s - loss: 0.8449 - accuracy: 0.7035 - val_loss: 0.8421 - val_accuracy: 0.7137 - 2s/epoch - 5ms/step\n",
      "Epoch 70/256\n",
      "391/391 - 2s - loss: 0.8406 - accuracy: 0.7057 - val_loss: 0.8335 - val_accuracy: 0.7107 - 2s/epoch - 5ms/step\n",
      "Epoch 71/256\n",
      "391/391 - 2s - loss: 0.8477 - accuracy: 0.7029 - val_loss: 0.8441 - val_accuracy: 0.7094 - 2s/epoch - 5ms/step\n",
      "Epoch 72/256\n",
      "391/391 - 2s - loss: 0.8491 - accuracy: 0.7052 - val_loss: 0.8351 - val_accuracy: 0.7150 - 2s/epoch - 5ms/step\n",
      "Epoch 73/256\n",
      "391/391 - 2s - loss: 0.8440 - accuracy: 0.7047 - val_loss: 0.8429 - val_accuracy: 0.7119 - 2s/epoch - 5ms/step\n",
      "Epoch 74/256\n",
      "391/391 - 2s - loss: 0.8402 - accuracy: 0.7051 - val_loss: 0.8550 - val_accuracy: 0.7056 - 2s/epoch - 5ms/step\n",
      "Epoch 75/256\n",
      "391/391 - 2s - loss: 0.8345 - accuracy: 0.7063 - val_loss: 0.8240 - val_accuracy: 0.7201 - 2s/epoch - 5ms/step\n",
      "Epoch 76/256\n",
      "391/391 - 2s - loss: 0.8353 - accuracy: 0.7085 - val_loss: 0.8297 - val_accuracy: 0.7176 - 2s/epoch - 5ms/step\n",
      "Epoch 77/256\n",
      "391/391 - 2s - loss: 0.8395 - accuracy: 0.7076 - val_loss: 0.8123 - val_accuracy: 0.7233 - 2s/epoch - 5ms/step\n",
      "Epoch 78/256\n",
      "391/391 - 2s - loss: 0.8350 - accuracy: 0.7069 - val_loss: 0.8321 - val_accuracy: 0.7114 - 2s/epoch - 5ms/step\n",
      "Epoch 79/256\n",
      "391/391 - 2s - loss: 0.8412 - accuracy: 0.7070 - val_loss: 0.8149 - val_accuracy: 0.7200 - 2s/epoch - 5ms/step\n",
      "Epoch 80/256\n",
      "391/391 - 2s - loss: 0.8346 - accuracy: 0.7086 - val_loss: 0.8243 - val_accuracy: 0.7150 - 2s/epoch - 5ms/step\n",
      "Epoch 81/256\n",
      "391/391 - 2s - loss: 0.8330 - accuracy: 0.7069 - val_loss: 0.8249 - val_accuracy: 0.7212 - 2s/epoch - 5ms/step\n",
      "Epoch 82/256\n",
      "391/391 - 2s - loss: 0.8273 - accuracy: 0.7085 - val_loss: 0.8444 - val_accuracy: 0.7101 - 2s/epoch - 5ms/step\n",
      "Epoch 83/256\n",
      "391/391 - 2s - loss: 0.8277 - accuracy: 0.7102 - val_loss: 0.8160 - val_accuracy: 0.7198 - 2s/epoch - 5ms/step\n",
      "Epoch 84/256\n",
      "391/391 - 2s - loss: 0.8293 - accuracy: 0.7095 - val_loss: 0.8251 - val_accuracy: 0.7203 - 2s/epoch - 5ms/step\n",
      "Epoch 85/256\n",
      "391/391 - 2s - loss: 0.8359 - accuracy: 0.7081 - val_loss: 0.8452 - val_accuracy: 0.7170 - 2s/epoch - 5ms/step\n",
      "Epoch 86/256\n",
      "391/391 - 2s - loss: 0.8255 - accuracy: 0.7127 - val_loss: 0.8129 - val_accuracy: 0.7229 - 2s/epoch - 5ms/step\n",
      "Epoch 87/256\n",
      "391/391 - 2s - loss: 0.8267 - accuracy: 0.7107 - val_loss: 0.8180 - val_accuracy: 0.7225 - 2s/epoch - 5ms/step\n",
      "Epoch 88/256\n",
      "391/391 - 2s - loss: 0.8321 - accuracy: 0.7093 - val_loss: 0.8112 - val_accuracy: 0.7216 - 2s/epoch - 5ms/step\n",
      "Epoch 89/256\n",
      "391/391 - 2s - loss: 0.8294 - accuracy: 0.7116 - val_loss: 0.8356 - val_accuracy: 0.7149 - 2s/epoch - 5ms/step\n",
      "Epoch 90/256\n",
      "391/391 - 2s - loss: 0.8245 - accuracy: 0.7142 - val_loss: 0.8084 - val_accuracy: 0.7258 - 2s/epoch - 5ms/step\n",
      "Epoch 91/256\n",
      "391/391 - 2s - loss: 0.8204 - accuracy: 0.7132 - val_loss: 0.8093 - val_accuracy: 0.7262 - 2s/epoch - 5ms/step\n",
      "Epoch 92/256\n",
      "391/391 - 2s - loss: 0.8311 - accuracy: 0.7101 - val_loss: 0.8123 - val_accuracy: 0.7263 - 2s/epoch - 5ms/step\n",
      "Epoch 93/256\n",
      "391/391 - 2s - loss: 0.8222 - accuracy: 0.7132 - val_loss: 0.8167 - val_accuracy: 0.7242 - 2s/epoch - 5ms/step\n",
      "Epoch 94/256\n",
      "391/391 - 2s - loss: 0.8119 - accuracy: 0.7159 - val_loss: 0.8240 - val_accuracy: 0.7225 - 2s/epoch - 5ms/step\n",
      "Epoch 95/256\n",
      "391/391 - 2s - loss: 0.8242 - accuracy: 0.7122 - val_loss: 0.8230 - val_accuracy: 0.7170 - 2s/epoch - 5ms/step\n",
      "Epoch 96/256\n",
      "391/391 - 2s - loss: 0.8114 - accuracy: 0.7155 - val_loss: 0.8055 - val_accuracy: 0.7233 - 2s/epoch - 5ms/step\n",
      "Epoch 97/256\n",
      "391/391 - 2s - loss: 0.8159 - accuracy: 0.7163 - val_loss: 0.8256 - val_accuracy: 0.7220 - 2s/epoch - 5ms/step\n",
      "Epoch 98/256\n",
      "391/391 - 2s - loss: 0.8194 - accuracy: 0.7129 - val_loss: 0.8227 - val_accuracy: 0.7180 - 2s/epoch - 5ms/step\n",
      "Epoch 99/256\n",
      "391/391 - 2s - loss: 0.8144 - accuracy: 0.7146 - val_loss: 0.8073 - val_accuracy: 0.7262 - 2s/epoch - 5ms/step\n",
      "Epoch 100/256\n",
      "391/391 - 2s - loss: 0.8138 - accuracy: 0.7151 - val_loss: 0.8111 - val_accuracy: 0.7219 - 2s/epoch - 5ms/step\n",
      "Epoch 101/256\n",
      "391/391 - 2s - loss: 0.8129 - accuracy: 0.7159 - val_loss: 0.8160 - val_accuracy: 0.7233 - 2s/epoch - 5ms/step\n",
      "Epoch 102/256\n",
      "391/391 - 2s - loss: 0.8171 - accuracy: 0.7167 - val_loss: 0.8011 - val_accuracy: 0.7230 - 2s/epoch - 5ms/step\n",
      "Epoch 103/256\n",
      "391/391 - 2s - loss: 0.8081 - accuracy: 0.7194 - val_loss: 0.8056 - val_accuracy: 0.7222 - 2s/epoch - 5ms/step\n",
      "Epoch 104/256\n",
      "391/391 - 2s - loss: 0.8081 - accuracy: 0.7170 - val_loss: 0.8114 - val_accuracy: 0.7272 - 2s/epoch - 5ms/step\n",
      "Epoch 105/256\n",
      "391/391 - 2s - loss: 0.8105 - accuracy: 0.7178 - val_loss: 0.8023 - val_accuracy: 0.7232 - 2s/epoch - 5ms/step\n",
      "Epoch 106/256\n",
      "391/391 - 2s - loss: 0.8065 - accuracy: 0.7158 - val_loss: 0.8216 - val_accuracy: 0.7184 - 2s/epoch - 5ms/step\n",
      "Epoch 107/256\n",
      "391/391 - 2s - loss: 0.8091 - accuracy: 0.7168 - val_loss: 0.8099 - val_accuracy: 0.7223 - 2s/epoch - 5ms/step\n",
      "Epoch 108/256\n",
      "391/391 - 2s - loss: 0.8123 - accuracy: 0.7192 - val_loss: 0.8231 - val_accuracy: 0.7178 - 2s/epoch - 5ms/step\n",
      "Epoch 109/256\n",
      "391/391 - 2s - loss: 0.8064 - accuracy: 0.7204 - val_loss: 0.8112 - val_accuracy: 0.7199 - 2s/epoch - 5ms/step\n",
      "Epoch 110/256\n",
      "391/391 - 2s - loss: 0.8023 - accuracy: 0.7210 - val_loss: 0.8234 - val_accuracy: 0.7248 - 2s/epoch - 5ms/step\n",
      "Epoch 111/256\n",
      "391/391 - 2s - loss: 0.8075 - accuracy: 0.7179 - val_loss: 0.8133 - val_accuracy: 0.7251 - 2s/epoch - 5ms/step\n",
      "Epoch 112/256\n",
      "391/391 - 2s - loss: 0.8005 - accuracy: 0.7209 - val_loss: 0.7987 - val_accuracy: 0.7278 - 2s/epoch - 5ms/step\n",
      "Epoch 113/256\n",
      "391/391 - 2s - loss: 0.8115 - accuracy: 0.7147 - val_loss: 0.8153 - val_accuracy: 0.7267 - 2s/epoch - 5ms/step\n",
      "Epoch 114/256\n",
      "391/391 - 2s - loss: 0.8040 - accuracy: 0.7202 - val_loss: 0.8106 - val_accuracy: 0.7203 - 2s/epoch - 5ms/step\n",
      "Epoch 115/256\n",
      "391/391 - 2s - loss: 0.8011 - accuracy: 0.7232 - val_loss: 0.8052 - val_accuracy: 0.7225 - 2s/epoch - 5ms/step\n",
      "Epoch 116/256\n",
      "391/391 - 2s - loss: 0.7963 - accuracy: 0.7187 - val_loss: 0.8094 - val_accuracy: 0.7230 - 2s/epoch - 5ms/step\n",
      "Epoch 117/256\n",
      "391/391 - 2s - loss: 0.7986 - accuracy: 0.7212 - val_loss: 0.8142 - val_accuracy: 0.7201 - 2s/epoch - 5ms/step\n",
      "Epoch 118/256\n",
      "391/391 - 2s - loss: 0.7983 - accuracy: 0.7211 - val_loss: 0.8022 - val_accuracy: 0.7242 - 2s/epoch - 5ms/step\n",
      "Epoch 119/256\n",
      "391/391 - 2s - loss: 0.8033 - accuracy: 0.7214 - val_loss: 0.8036 - val_accuracy: 0.7243 - 2s/epoch - 5ms/step\n",
      "Epoch 120/256\n",
      "391/391 - 2s - loss: 0.8069 - accuracy: 0.7197 - val_loss: 0.8026 - val_accuracy: 0.7222 - 2s/epoch - 5ms/step\n",
      "Epoch 121/256\n",
      "391/391 - 2s - loss: 0.8053 - accuracy: 0.7191 - val_loss: 0.8247 - val_accuracy: 0.7133 - 2s/epoch - 6ms/step\n",
      "Epoch 122/256\n",
      "391/391 - 2s - loss: 0.8025 - accuracy: 0.7204 - val_loss: 0.8004 - val_accuracy: 0.7279 - 2s/epoch - 5ms/step\n",
      "Epoch 123/256\n",
      "391/391 - 2s - loss: 0.8088 - accuracy: 0.7179 - val_loss: 0.7968 - val_accuracy: 0.7248 - 2s/epoch - 5ms/step\n",
      "Epoch 124/256\n",
      "391/391 - 2s - loss: 0.7982 - accuracy: 0.7197 - val_loss: 0.8148 - val_accuracy: 0.7239 - 2s/epoch - 5ms/step\n",
      "Epoch 125/256\n",
      "391/391 - 2s - loss: 0.7966 - accuracy: 0.7208 - val_loss: 0.8098 - val_accuracy: 0.7202 - 2s/epoch - 5ms/step\n",
      "Epoch 126/256\n",
      "391/391 - 2s - loss: 0.8002 - accuracy: 0.7204 - val_loss: 0.8207 - val_accuracy: 0.7205 - 2s/epoch - 5ms/step\n",
      "Epoch 127/256\n",
      "391/391 - 2s - loss: 0.8012 - accuracy: 0.7210 - val_loss: 0.8011 - val_accuracy: 0.7259 - 2s/epoch - 5ms/step\n",
      "Epoch 128/256\n",
      "391/391 - 2s - loss: 0.7947 - accuracy: 0.7238 - val_loss: 0.8384 - val_accuracy: 0.7109 - 2s/epoch - 5ms/step\n",
      "Epoch 129/256\n",
      "391/391 - 2s - loss: 0.8013 - accuracy: 0.7190 - val_loss: 0.8138 - val_accuracy: 0.7203 - 2s/epoch - 5ms/step\n",
      "Epoch 130/256\n",
      "391/391 - 2s - loss: 0.8006 - accuracy: 0.7215 - val_loss: 0.8225 - val_accuracy: 0.7173 - 2s/epoch - 5ms/step\n",
      "Epoch 131/256\n",
      "391/391 - 2s - loss: 0.7964 - accuracy: 0.7218 - val_loss: 0.7999 - val_accuracy: 0.7267 - 2s/epoch - 5ms/step\n",
      "Epoch 132/256\n",
      "391/391 - 2s - loss: 0.7899 - accuracy: 0.7228 - val_loss: 0.7911 - val_accuracy: 0.7336 - 2s/epoch - 5ms/step\n",
      "Epoch 133/256\n",
      "391/391 - 2s - loss: 0.7978 - accuracy: 0.7221 - val_loss: 0.7956 - val_accuracy: 0.7255 - 2s/epoch - 5ms/step\n",
      "Epoch 134/256\n",
      "391/391 - 2s - loss: 0.7988 - accuracy: 0.7221 - val_loss: 0.8067 - val_accuracy: 0.7225 - 2s/epoch - 5ms/step\n",
      "Epoch 135/256\n",
      "391/391 - 2s - loss: 0.7851 - accuracy: 0.7270 - val_loss: 0.7936 - val_accuracy: 0.7290 - 2s/epoch - 5ms/step\n",
      "Epoch 136/256\n",
      "391/391 - 2s - loss: 0.7948 - accuracy: 0.7249 - val_loss: 0.7910 - val_accuracy: 0.7314 - 2s/epoch - 5ms/step\n",
      "Epoch 137/256\n",
      "391/391 - 2s - loss: 0.7918 - accuracy: 0.7255 - val_loss: 0.8054 - val_accuracy: 0.7230 - 2s/epoch - 5ms/step\n",
      "Epoch 138/256\n",
      "391/391 - 2s - loss: 0.7873 - accuracy: 0.7263 - val_loss: 0.8023 - val_accuracy: 0.7248 - 2s/epoch - 5ms/step\n",
      "Epoch 139/256\n",
      "391/391 - 2s - loss: 0.7869 - accuracy: 0.7288 - val_loss: 0.7986 - val_accuracy: 0.7256 - 2s/epoch - 5ms/step\n",
      "Epoch 140/256\n",
      "391/391 - 2s - loss: 0.7930 - accuracy: 0.7235 - val_loss: 0.8150 - val_accuracy: 0.7189 - 2s/epoch - 5ms/step\n",
      "Epoch 141/256\n",
      "391/391 - 2s - loss: 0.7961 - accuracy: 0.7242 - val_loss: 0.8110 - val_accuracy: 0.7227 - 2s/epoch - 5ms/step\n",
      "Epoch 142/256\n",
      "391/391 - 2s - loss: 0.7866 - accuracy: 0.7258 - val_loss: 0.7842 - val_accuracy: 0.7317 - 2s/epoch - 5ms/step\n",
      "Epoch 143/256\n",
      "391/391 - 2s - loss: 0.7945 - accuracy: 0.7225 - val_loss: 0.8020 - val_accuracy: 0.7233 - 2s/epoch - 5ms/step\n",
      "Epoch 144/256\n",
      "391/391 - 2s - loss: 0.7866 - accuracy: 0.7247 - val_loss: 0.8089 - val_accuracy: 0.7259 - 2s/epoch - 5ms/step\n",
      "Epoch 145/256\n",
      "391/391 - 2s - loss: 0.7836 - accuracy: 0.7272 - val_loss: 0.7946 - val_accuracy: 0.7316 - 2s/epoch - 5ms/step\n",
      "Epoch 146/256\n",
      "391/391 - 2s - loss: 0.7908 - accuracy: 0.7247 - val_loss: 0.7858 - val_accuracy: 0.7351 - 2s/epoch - 5ms/step\n",
      "Epoch 147/256\n",
      "391/391 - 2s - loss: 0.7860 - accuracy: 0.7241 - val_loss: 0.8021 - val_accuracy: 0.7268 - 2s/epoch - 5ms/step\n",
      "Epoch 148/256\n",
      "391/391 - 2s - loss: 0.7849 - accuracy: 0.7267 - val_loss: 0.8030 - val_accuracy: 0.7271 - 2s/epoch - 5ms/step\n",
      "Epoch 149/256\n",
      "391/391 - 2s - loss: 0.7828 - accuracy: 0.7264 - val_loss: 0.7813 - val_accuracy: 0.7337 - 2s/epoch - 5ms/step\n",
      "Epoch 150/256\n",
      "391/391 - 2s - loss: 0.7786 - accuracy: 0.7302 - val_loss: 0.7923 - val_accuracy: 0.7322 - 2s/epoch - 5ms/step\n",
      "Epoch 151/256\n",
      "391/391 - 2s - loss: 0.7857 - accuracy: 0.7309 - val_loss: 0.7891 - val_accuracy: 0.7323 - 2s/epoch - 5ms/step\n",
      "Epoch 152/256\n",
      "391/391 - 2s - loss: 0.7862 - accuracy: 0.7275 - val_loss: 0.8060 - val_accuracy: 0.7230 - 2s/epoch - 5ms/step\n",
      "Epoch 153/256\n",
      "391/391 - 2s - loss: 0.7909 - accuracy: 0.7257 - val_loss: 0.7944 - val_accuracy: 0.7297 - 2s/epoch - 5ms/step\n",
      "Epoch 154/256\n",
      "391/391 - 2s - loss: 0.7757 - accuracy: 0.7289 - val_loss: 0.7907 - val_accuracy: 0.7306 - 2s/epoch - 5ms/step\n",
      "Epoch 155/256\n",
      "391/391 - 2s - loss: 0.7814 - accuracy: 0.7273 - val_loss: 0.7858 - val_accuracy: 0.7316 - 2s/epoch - 5ms/step\n",
      "Epoch 156/256\n",
      "391/391 - 2s - loss: 0.7869 - accuracy: 0.7267 - val_loss: 0.7956 - val_accuracy: 0.7283 - 2s/epoch - 5ms/step\n",
      "Epoch 157/256\n",
      "391/391 - 2s - loss: 0.7744 - accuracy: 0.7313 - val_loss: 0.8041 - val_accuracy: 0.7261 - 2s/epoch - 5ms/step\n",
      "Epoch 158/256\n",
      "391/391 - 2s - loss: 0.7851 - accuracy: 0.7277 - val_loss: 0.7872 - val_accuracy: 0.7302 - 2s/epoch - 5ms/step\n",
      "Epoch 159/256\n",
      "391/391 - 2s - loss: 0.7815 - accuracy: 0.7277 - val_loss: 0.7940 - val_accuracy: 0.7283 - 2s/epoch - 5ms/step\n",
      "Epoch 160/256\n",
      "391/391 - 2s - loss: 0.7782 - accuracy: 0.7268 - val_loss: 0.7875 - val_accuracy: 0.7323 - 2s/epoch - 5ms/step\n",
      "Epoch 161/256\n",
      "391/391 - 2s - loss: 0.7775 - accuracy: 0.7283 - val_loss: 0.7864 - val_accuracy: 0.7360 - 2s/epoch - 5ms/step\n",
      "Epoch 162/256\n",
      "391/391 - 2s - loss: 0.7806 - accuracy: 0.7277 - val_loss: 0.7893 - val_accuracy: 0.7307 - 2s/epoch - 5ms/step\n",
      "Epoch 163/256\n",
      "391/391 - 2s - loss: 0.7793 - accuracy: 0.7298 - val_loss: 0.7921 - val_accuracy: 0.7294 - 2s/epoch - 5ms/step\n",
      "Epoch 164/256\n",
      "391/391 - 2s - loss: 0.7855 - accuracy: 0.7251 - val_loss: 0.7873 - val_accuracy: 0.7358 - 2s/epoch - 5ms/step\n",
      "Epoch 165/256\n",
      "391/391 - 2s - loss: 0.7803 - accuracy: 0.7274 - val_loss: 0.8014 - val_accuracy: 0.7283 - 2s/epoch - 5ms/step\n",
      "Epoch 166/256\n",
      "391/391 - 2s - loss: 0.7698 - accuracy: 0.7329 - val_loss: 0.7938 - val_accuracy: 0.7269 - 2s/epoch - 5ms/step\n",
      "Epoch 167/256\n",
      "391/391 - 2s - loss: 0.7791 - accuracy: 0.7291 - val_loss: 0.7815 - val_accuracy: 0.7362 - 2s/epoch - 5ms/step\n",
      "Epoch 168/256\n",
      "391/391 - 2s - loss: 0.7776 - accuracy: 0.7286 - val_loss: 0.7773 - val_accuracy: 0.7360 - 2s/epoch - 5ms/step\n",
      "Epoch 169/256\n",
      "391/391 - 2s - loss: 0.7791 - accuracy: 0.7274 - val_loss: 0.7947 - val_accuracy: 0.7246 - 2s/epoch - 5ms/step\n",
      "Epoch 170/256\n",
      "391/391 - 2s - loss: 0.7815 - accuracy: 0.7277 - val_loss: 0.8021 - val_accuracy: 0.7295 - 2s/epoch - 5ms/step\n",
      "Epoch 171/256\n",
      "391/391 - 2s - loss: 0.7805 - accuracy: 0.7290 - val_loss: 0.7748 - val_accuracy: 0.7399 - 2s/epoch - 5ms/step\n",
      "Epoch 172/256\n",
      "391/391 - 2s - loss: 0.7765 - accuracy: 0.7302 - val_loss: 0.8039 - val_accuracy: 0.7280 - 2s/epoch - 5ms/step\n",
      "Epoch 173/256\n",
      "391/391 - 2s - loss: 0.7703 - accuracy: 0.7319 - val_loss: 0.7914 - val_accuracy: 0.7295 - 2s/epoch - 5ms/step\n",
      "Epoch 174/256\n",
      "391/391 - 2s - loss: 0.7754 - accuracy: 0.7299 - val_loss: 0.7965 - val_accuracy: 0.7265 - 2s/epoch - 5ms/step\n",
      "Epoch 175/256\n",
      "391/391 - 2s - loss: 0.7739 - accuracy: 0.7289 - val_loss: 0.7871 - val_accuracy: 0.7302 - 2s/epoch - 5ms/step\n",
      "Epoch 176/256\n",
      "391/391 - 2s - loss: 0.7730 - accuracy: 0.7295 - val_loss: 0.7864 - val_accuracy: 0.7334 - 2s/epoch - 5ms/step\n",
      "Epoch 177/256\n",
      "391/391 - 2s - loss: 0.7766 - accuracy: 0.7317 - val_loss: 0.7794 - val_accuracy: 0.7337 - 2s/epoch - 5ms/step\n",
      "Epoch 178/256\n",
      "391/391 - 2s - loss: 0.7698 - accuracy: 0.7314 - val_loss: 0.8156 - val_accuracy: 0.7222 - 2s/epoch - 5ms/step\n",
      "Epoch 179/256\n",
      "391/391 - 2s - loss: 0.7685 - accuracy: 0.7328 - val_loss: 0.7830 - val_accuracy: 0.7325 - 2s/epoch - 5ms/step\n",
      "Epoch 180/256\n",
      "391/391 - 2s - loss: 0.7742 - accuracy: 0.7314 - val_loss: 0.7829 - val_accuracy: 0.7329 - 2s/epoch - 5ms/step\n",
      "Epoch 181/256\n",
      "391/391 - 2s - loss: 0.7726 - accuracy: 0.7313 - val_loss: 0.7860 - val_accuracy: 0.7337 - 2s/epoch - 5ms/step\n",
      "Epoch 182/256\n",
      "391/391 - 2s - loss: 0.7660 - accuracy: 0.7334 - val_loss: 0.7723 - val_accuracy: 0.7364 - 2s/epoch - 5ms/step\n",
      "Epoch 183/256\n",
      "391/391 - 2s - loss: 0.7698 - accuracy: 0.7330 - val_loss: 0.7844 - val_accuracy: 0.7342 - 2s/epoch - 5ms/step\n",
      "Epoch 184/256\n",
      "391/391 - 2s - loss: 0.7704 - accuracy: 0.7316 - val_loss: 0.7830 - val_accuracy: 0.7335 - 2s/epoch - 5ms/step\n",
      "Epoch 185/256\n",
      "391/391 - 2s - loss: 0.7676 - accuracy: 0.7323 - val_loss: 0.7883 - val_accuracy: 0.7322 - 2s/epoch - 5ms/step\n",
      "Epoch 186/256\n",
      "391/391 - 2s - loss: 0.7697 - accuracy: 0.7319 - val_loss: 0.7749 - val_accuracy: 0.7359 - 2s/epoch - 5ms/step\n",
      "Epoch 187/256\n",
      "391/391 - 2s - loss: 0.7732 - accuracy: 0.7308 - val_loss: 0.7964 - val_accuracy: 0.7299 - 2s/epoch - 5ms/step\n",
      "Epoch 188/256\n",
      "391/391 - 2s - loss: 0.7751 - accuracy: 0.7331 - val_loss: 0.7770 - val_accuracy: 0.7354 - 2s/epoch - 5ms/step\n",
      "Epoch 189/256\n",
      "391/391 - 2s - loss: 0.7628 - accuracy: 0.7342 - val_loss: 0.8023 - val_accuracy: 0.7195 - 2s/epoch - 5ms/step\n",
      "Epoch 190/256\n",
      "391/391 - 2s - loss: 0.7709 - accuracy: 0.7313 - val_loss: 0.7732 - val_accuracy: 0.7403 - 2s/epoch - 5ms/step\n",
      "Epoch 191/256\n",
      "391/391 - 2s - loss: 0.7696 - accuracy: 0.7302 - val_loss: 0.7838 - val_accuracy: 0.7343 - 2s/epoch - 5ms/step\n",
      "Epoch 192/256\n",
      "391/391 - 2s - loss: 0.7647 - accuracy: 0.7355 - val_loss: 0.7856 - val_accuracy: 0.7345 - 2s/epoch - 5ms/step\n",
      "Epoch 193/256\n",
      "391/391 - 2s - loss: 0.7689 - accuracy: 0.7343 - val_loss: 0.8156 - val_accuracy: 0.7201 - 2s/epoch - 5ms/step\n",
      "Epoch 194/256\n",
      "391/391 - 2s - loss: 0.7662 - accuracy: 0.7327 - val_loss: 0.7860 - val_accuracy: 0.7312 - 2s/epoch - 5ms/step\n",
      "Epoch 195/256\n",
      "391/391 - 2s - loss: 0.7685 - accuracy: 0.7325 - val_loss: 0.7703 - val_accuracy: 0.7383 - 2s/epoch - 5ms/step\n",
      "Epoch 196/256\n",
      "391/391 - 2s - loss: 0.7641 - accuracy: 0.7334 - val_loss: 0.7887 - val_accuracy: 0.7315 - 2s/epoch - 5ms/step\n",
      "Epoch 197/256\n",
      "391/391 - 2s - loss: 0.7724 - accuracy: 0.7333 - val_loss: 0.7904 - val_accuracy: 0.7328 - 2s/epoch - 5ms/step\n",
      "Epoch 198/256\n",
      "391/391 - 2s - loss: 0.7602 - accuracy: 0.7367 - val_loss: 0.7811 - val_accuracy: 0.7372 - 2s/epoch - 5ms/step\n",
      "Epoch 199/256\n",
      "391/391 - 2s - loss: 0.7640 - accuracy: 0.7346 - val_loss: 0.7975 - val_accuracy: 0.7274 - 2s/epoch - 5ms/step\n",
      "Epoch 200/256\n",
      "391/391 - 2s - loss: 0.7684 - accuracy: 0.7319 - val_loss: 0.7912 - val_accuracy: 0.7350 - 2s/epoch - 5ms/step\n",
      "Epoch 201/256\n",
      "391/391 - 2s - loss: 0.7654 - accuracy: 0.7329 - val_loss: 0.7873 - val_accuracy: 0.7296 - 2s/epoch - 5ms/step\n",
      "Epoch 202/256\n",
      "391/391 - 2s - loss: 0.7639 - accuracy: 0.7329 - val_loss: 0.7781 - val_accuracy: 0.7394 - 2s/epoch - 5ms/step\n",
      "Epoch 203/256\n",
      "391/391 - 2s - loss: 0.7581 - accuracy: 0.7371 - val_loss: 0.7948 - val_accuracy: 0.7260 - 2s/epoch - 5ms/step\n",
      "Epoch 204/256\n",
      "391/391 - 2s - loss: 0.7650 - accuracy: 0.7356 - val_loss: 0.8048 - val_accuracy: 0.7272 - 2s/epoch - 5ms/step\n",
      "Epoch 205/256\n",
      "391/391 - 2s - loss: 0.7637 - accuracy: 0.7341 - val_loss: 0.8199 - val_accuracy: 0.7214 - 2s/epoch - 5ms/step\n",
      "Epoch 206/256\n",
      "391/391 - 2s - loss: 0.7681 - accuracy: 0.7345 - val_loss: 0.7775 - val_accuracy: 0.7346 - 2s/epoch - 5ms/step\n",
      "Epoch 207/256\n",
      "391/391 - 2s - loss: 0.7595 - accuracy: 0.7347 - val_loss: 0.7723 - val_accuracy: 0.7373 - 2s/epoch - 5ms/step\n",
      "Epoch 208/256\n",
      "391/391 - 2s - loss: 0.7629 - accuracy: 0.7351 - val_loss: 0.7828 - val_accuracy: 0.7337 - 2s/epoch - 5ms/step\n",
      "Epoch 209/256\n",
      "391/391 - 2s - loss: 0.7603 - accuracy: 0.7366 - val_loss: 0.7707 - val_accuracy: 0.7389 - 2s/epoch - 5ms/step\n",
      "Epoch 210/256\n",
      "391/391 - 2s - loss: 0.7608 - accuracy: 0.7345 - val_loss: 0.7821 - val_accuracy: 0.7320 - 2s/epoch - 5ms/step\n",
      "Epoch 211/256\n",
      "391/391 - 2s - loss: 0.7639 - accuracy: 0.7352 - val_loss: 0.7993 - val_accuracy: 0.7324 - 2s/epoch - 5ms/step\n",
      "Epoch 212/256\n",
      "391/391 - 2s - loss: 0.7620 - accuracy: 0.7339 - val_loss: 0.7798 - val_accuracy: 0.7335 - 2s/epoch - 5ms/step\n",
      "Epoch 213/256\n",
      "391/391 - 2s - loss: 0.7606 - accuracy: 0.7369 - val_loss: 0.7704 - val_accuracy: 0.7398 - 2s/epoch - 5ms/step\n",
      "Epoch 214/256\n",
      "391/391 - 2s - loss: 0.7624 - accuracy: 0.7352 - val_loss: 0.7819 - val_accuracy: 0.7365 - 2s/epoch - 5ms/step\n",
      "Epoch 215/256\n",
      "391/391 - 2s - loss: 0.7598 - accuracy: 0.7368 - val_loss: 0.8088 - val_accuracy: 0.7285 - 2s/epoch - 5ms/step\n",
      "Epoch 216/256\n",
      "391/391 - 2s - loss: 0.7648 - accuracy: 0.7336 - val_loss: 0.7792 - val_accuracy: 0.7356 - 2s/epoch - 5ms/step\n",
      "Epoch 217/256\n",
      "391/391 - 2s - loss: 0.7597 - accuracy: 0.7356 - val_loss: 0.7734 - val_accuracy: 0.7356 - 2s/epoch - 5ms/step\n",
      "Epoch 218/256\n",
      "391/391 - 2s - loss: 0.7595 - accuracy: 0.7361 - val_loss: 0.7826 - val_accuracy: 0.7407 - 2s/epoch - 5ms/step\n",
      "Epoch 219/256\n",
      "391/391 - 2s - loss: 0.7575 - accuracy: 0.7363 - val_loss: 0.7716 - val_accuracy: 0.7383 - 2s/epoch - 5ms/step\n",
      "Epoch 220/256\n",
      "391/391 - 2s - loss: 0.7518 - accuracy: 0.7392 - val_loss: 0.7738 - val_accuracy: 0.7376 - 2s/epoch - 5ms/step\n",
      "Epoch 221/256\n",
      "391/391 - 2s - loss: 0.7599 - accuracy: 0.7363 - val_loss: 0.7716 - val_accuracy: 0.7423 - 2s/epoch - 5ms/step\n",
      "Epoch 222/256\n",
      "391/391 - 2s - loss: 0.7627 - accuracy: 0.7368 - val_loss: 0.7696 - val_accuracy: 0.7345 - 2s/epoch - 5ms/step\n",
      "Epoch 223/256\n",
      "391/391 - 2s - loss: 0.7585 - accuracy: 0.7351 - val_loss: 0.7793 - val_accuracy: 0.7377 - 2s/epoch - 5ms/step\n",
      "Epoch 224/256\n",
      "391/391 - 2s - loss: 0.7569 - accuracy: 0.7370 - val_loss: 0.7870 - val_accuracy: 0.7283 - 2s/epoch - 6ms/step\n",
      "Epoch 225/256\n",
      "391/391 - 2s - loss: 0.7569 - accuracy: 0.7388 - val_loss: 0.7747 - val_accuracy: 0.7341 - 2s/epoch - 5ms/step\n",
      "Epoch 226/256\n",
      "391/391 - 2s - loss: 0.7597 - accuracy: 0.7365 - val_loss: 0.7911 - val_accuracy: 0.7264 - 2s/epoch - 5ms/step\n",
      "Epoch 227/256\n",
      "391/391 - 2s - loss: 0.7558 - accuracy: 0.7359 - val_loss: 0.8008 - val_accuracy: 0.7316 - 2s/epoch - 5ms/step\n",
      "Epoch 228/256\n",
      "391/391 - 2s - loss: 0.7541 - accuracy: 0.7383 - val_loss: 0.7694 - val_accuracy: 0.7412 - 2s/epoch - 5ms/step\n",
      "Epoch 229/256\n",
      "391/391 - 2s - loss: 0.7557 - accuracy: 0.7375 - val_loss: 0.7797 - val_accuracy: 0.7330 - 2s/epoch - 5ms/step\n",
      "Epoch 230/256\n",
      "391/391 - 2s - loss: 0.7489 - accuracy: 0.7413 - val_loss: 0.7792 - val_accuracy: 0.7364 - 2s/epoch - 5ms/step\n",
      "Epoch 231/256\n",
      "391/391 - 2s - loss: 0.7593 - accuracy: 0.7355 - val_loss: 0.7784 - val_accuracy: 0.7344 - 2s/epoch - 5ms/step\n",
      "Epoch 232/256\n",
      "391/391 - 2s - loss: 0.7503 - accuracy: 0.7397 - val_loss: 0.7781 - val_accuracy: 0.7378 - 2s/epoch - 5ms/step\n",
      "Epoch 233/256\n",
      "391/391 - 2s - loss: 0.7615 - accuracy: 0.7360 - val_loss: 0.7675 - val_accuracy: 0.7430 - 2s/epoch - 5ms/step\n",
      "Epoch 234/256\n",
      "391/391 - 2s - loss: 0.7577 - accuracy: 0.7385 - val_loss: 0.7751 - val_accuracy: 0.7339 - 2s/epoch - 5ms/step\n",
      "Epoch 235/256\n",
      "391/391 - 2s - loss: 0.7592 - accuracy: 0.7362 - val_loss: 0.7741 - val_accuracy: 0.7391 - 2s/epoch - 5ms/step\n",
      "Epoch 236/256\n",
      "391/391 - 2s - loss: 0.7615 - accuracy: 0.7350 - val_loss: 0.7817 - val_accuracy: 0.7324 - 2s/epoch - 5ms/step\n",
      "Epoch 237/256\n",
      "391/391 - 2s - loss: 0.7563 - accuracy: 0.7377 - val_loss: 0.7872 - val_accuracy: 0.7309 - 2s/epoch - 5ms/step\n",
      "Epoch 238/256\n",
      "391/391 - 2s - loss: 0.7557 - accuracy: 0.7376 - val_loss: 0.7749 - val_accuracy: 0.7374 - 2s/epoch - 5ms/step\n",
      "Epoch 239/256\n",
      "391/391 - 2s - loss: 0.7533 - accuracy: 0.7389 - val_loss: 0.7769 - val_accuracy: 0.7394 - 2s/epoch - 5ms/step\n",
      "Epoch 240/256\n",
      "391/391 - 2s - loss: 0.7586 - accuracy: 0.7375 - val_loss: 0.7767 - val_accuracy: 0.7367 - 2s/epoch - 5ms/step\n",
      "Epoch 241/256\n",
      "391/391 - 2s - loss: 0.7584 - accuracy: 0.7390 - val_loss: 0.7795 - val_accuracy: 0.7376 - 2s/epoch - 5ms/step\n",
      "Epoch 242/256\n",
      "391/391 - 2s - loss: 0.7504 - accuracy: 0.7392 - val_loss: 0.7764 - val_accuracy: 0.7360 - 2s/epoch - 5ms/step\n",
      "Epoch 243/256\n",
      "391/391 - 2s - loss: 0.7508 - accuracy: 0.7414 - val_loss: 0.7815 - val_accuracy: 0.7361 - 2s/epoch - 5ms/step\n",
      "Epoch 244/256\n",
      "391/391 - 2s - loss: 0.7553 - accuracy: 0.7372 - val_loss: 0.7813 - val_accuracy: 0.7394 - 2s/epoch - 5ms/step\n",
      "Epoch 245/256\n",
      "391/391 - 2s - loss: 0.7567 - accuracy: 0.7384 - val_loss: 0.7702 - val_accuracy: 0.7379 - 2s/epoch - 5ms/step\n",
      "Epoch 246/256\n",
      "391/391 - 2s - loss: 0.7603 - accuracy: 0.7383 - val_loss: 0.7834 - val_accuracy: 0.7299 - 2s/epoch - 5ms/step\n",
      "Epoch 247/256\n",
      "391/391 - 2s - loss: 0.7465 - accuracy: 0.7402 - val_loss: 0.7786 - val_accuracy: 0.7318 - 2s/epoch - 5ms/step\n",
      "Epoch 248/256\n",
      "391/391 - 2s - loss: 0.7533 - accuracy: 0.7386 - val_loss: 0.7918 - val_accuracy: 0.7350 - 2s/epoch - 5ms/step\n",
      "Epoch 249/256\n",
      "391/391 - 2s - loss: 0.7498 - accuracy: 0.7408 - val_loss: 0.7695 - val_accuracy: 0.7400 - 2s/epoch - 5ms/step\n",
      "Epoch 250/256\n",
      "391/391 - 2s - loss: 0.7575 - accuracy: 0.7383 - val_loss: 0.7773 - val_accuracy: 0.7323 - 2s/epoch - 5ms/step\n",
      "Epoch 251/256\n",
      "391/391 - 2s - loss: 0.7487 - accuracy: 0.7408 - val_loss: 0.7787 - val_accuracy: 0.7402 - 2s/epoch - 5ms/step\n",
      "Epoch 252/256\n",
      "391/391 - 2s - loss: 0.7474 - accuracy: 0.7418 - val_loss: 0.7737 - val_accuracy: 0.7341 - 2s/epoch - 5ms/step\n",
      "Epoch 253/256\n",
      "391/391 - 2s - loss: 0.7504 - accuracy: 0.7388 - val_loss: 0.7891 - val_accuracy: 0.7314 - 2s/epoch - 5ms/step\n",
      "Epoch 254/256\n",
      "391/391 - 2s - loss: 0.7538 - accuracy: 0.7380 - val_loss: 0.7707 - val_accuracy: 0.7384 - 2s/epoch - 5ms/step\n",
      "Epoch 255/256\n",
      "391/391 - 2s - loss: 0.7504 - accuracy: 0.7413 - val_loss: 0.7882 - val_accuracy: 0.7319 - 2s/epoch - 5ms/step\n",
      "Epoch 256/256\n",
      "391/391 - 2s - loss: 0.7478 - accuracy: 0.7386 - val_loss: 0.7683 - val_accuracy: 0.7404 - 2s/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47d62b5850>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=Adam(lr=0.001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
    "\n",
    "model.fit( x_train[0:50000], y_train[0:50000],\n",
    "          batch_size=128, epochs=256, verbose=2,\n",
    "          validation_data=(x_test[0:10000], y_test[0:10000]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h88y8nUa7wF6",
    "outputId": "bf346ab8-591b-457e-d5f2-1982a3030ef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "Accuracy Score : 0.7404\n",
      "Averaged Precision Score : 0.7444662486619341\n",
      "Averaged Recall Score : 0.7404\n",
      "Averaged F1 Score : 0.7406426862697927\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77      1000\n",
      "           1       0.85      0.83      0.84      1000\n",
      "           2       0.66      0.66      0.66      1000\n",
      "           3       0.53      0.61      0.57      1000\n",
      "           4       0.75      0.67      0.71      1000\n",
      "           5       0.67      0.60      0.64      1000\n",
      "           6       0.80      0.81      0.80      1000\n",
      "           7       0.86      0.71      0.78      1000\n",
      "           8       0.80      0.86      0.83      1000\n",
      "           9       0.79      0.83      0.81      1000\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.74      0.74      0.74     10000\n",
      "weighted avg       0.74      0.74      0.74     10000\n",
      "\n",
      "[0.7682916522026062, 0.7404000163078308]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = np.argmax(y_test[0:10000], axis=1)\n",
    "prediction = model.predict(x_test[0:10000])\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "\n",
    "metrics = { 'Accuracy Score' : metrics.accuracy_score(y_true, prediction),\n",
    "            'Averaged Precision Score' : metrics.precision_score(y_true, prediction, average='weighted'),\n",
    "            'Averaged Recall Score' : metrics.recall_score(y_true, prediction, average='weighted'),\n",
    "            'Averaged F1 Score' : metrics.f1_score(y_true, prediction, average='weighted')}\n",
    "for k,v in metrics.items():\n",
    "  print(f'{k} : {v}')\n",
    "\n",
    "print(classification_report(y_true, prediction))\n",
    "print( model.evaluate(x_test[0:10000], y_test[0:10000], verbose=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTDC8nT_7yu-",
    "outputId": "5574361a-cc92-441e-d640-76705f686ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photograph #1271... Actual Classification: 8 -> Predicted Classification: 8\n",
      "Photograph #5009... Actual Classification: 7 -> Predicted Classification: 2\n",
      "Photograph #2289... Actual Classification: 1 -> Predicted Classification: 1\n",
      "Photograph #8132... Actual Classification: 8 -> Predicted Classification: 8\n",
      "Photograph #9194... Actual Classification: 6 -> Predicted Classification: 6\n",
      "Photograph #9810... Actual Classification: 8 -> Predicted Classification: 8\n",
      "Photograph #856... Actual Classification: 7 -> Predicted Classification: 7\n",
      "Photograph #9211... Actual Classification: 1 -> Predicted Classification: 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 8):\n",
    "  idx = np.random.randint(1,prediction.shape[0])\n",
    "  print(f'Photograph #{idx}... Actual Classification: {y_true[idx]} -> Predicted Classification: {prediction[idx]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjcQwvRHPAfo"
   },
   "source": [
    "## Part II: CNN model with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jpYm08zPFC7",
    "outputId": "b89361fd-9533-4ac4-8a1a-deafceeb94ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# We load data again.   The data split between train and test sets:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJ1nsArfPHUH"
   },
   "source": [
    "### Here we would like to use one pre-trained model called VGG16.   For more details on VGG16, please go to https://neurohive.io/en/popular-networks/vgg16/\n",
    "\n",
    "\n",
    "### VGG16 supports down to 48x48 images as an input. However, the resolution of our images is too low, which is (32, 32) so we need to increase the resolution.   This is called upsampling. \n",
    "\n",
    "\n",
    "\n",
    "### Find a way to do upsampling for each image to increase its resolution from 32x32 to 64x64. One option is to use the function resize(), which is provided by scikit-image library (https://scikit-image.org/)\n",
    "\n",
    "\n",
    "### Hints: \n",
    "\n",
    "#### (1) If you use resize() in scikit-image, that function also normalizes the input image so you may not want to normalize twice.\n",
    "\n",
    "#### Learn from the examples here:  https://scikit-image.org/docs/stable/auto_examples/transform/plot_rescale.html\n",
    "\n",
    "#### (2) Apply upsampling to x_train and x_test seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTXN9CGcPKT4",
    "outputId": "ddc3fee7-ec13-4e95-ef14-5bef8c3e9800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import skimage.transform\n",
    "\n",
    "#hint: use np.zeros() to initialize an all-zero numpy array with desirable size and then assign each resized image into it\n",
    "\n",
    "new_x_train = np.zeros((50000,64,64,3))\n",
    "\n",
    "print(new_x_train.shape)\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "  newImage = skimage.transform.resize( x_train[i], (64,64) )\n",
    "  new_x_train[i] = newImage\n",
    "\n",
    "#for image in x_train:\n",
    "#  newImage = skimage.transform.resize(image, (64, 64))      # note that resize() also normalizes your image\n",
    "#  new_x_train = newImage\n",
    "\n",
    "# this process may take about a few minutes ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJLkwkmYPMi6"
   },
   "outputs": [],
   "source": [
    "#new_x_train = np.asarray(new_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "M9KD8s6rLMa1",
    "outputId": "8aa05acd-fd17-48d0-f1f7-a9a9748ac240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2e1c3ba90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(new_x_train.shape)\n",
    "\n",
    "idx = np.random.randint(1,x_train.shape[0])\n",
    "plt.imshow(new_x_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PvHegBPLOLn"
   },
   "outputs": [],
   "source": [
    "new_x_test = np.zeros((10000,64,64,3))\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "  newImage = skimage.transform.resize( x_test[i], (64,64) )\n",
    "  new_x_test[i] = newImage\n",
    "\n",
    "#for image in x_test:\n",
    "#  newImage = skimage.transform.resize(image, (64, 64))\n",
    "#  new_x_test = newImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JJOka5wLVIv"
   },
   "outputs": [],
   "source": [
    "#new_x_test = np.asarray(new_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "V97CDXMRLVry",
    "outputId": "dfef0b89-0383-4715-a0ec-a29db3dbdbbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2e169eb90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(new_x_test.shape)\n",
    "\n",
    "idx = np.random.randint(1,x_test.shape[0])\n",
    "plt.imshow(new_x_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqapBPRNPO5d"
   },
   "source": [
    "### Write your code in the cell below to do the following:\n",
    "\n",
    "- First convert y_train and y_test from 2D to 1D by using reshape() function \n",
    "- Next apply one-hot encoding to y_train and y_test by using tf.keras.utils.to_categorical() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rati5FMgPQkD"
   },
   "outputs": [],
   "source": [
    "# Convert y_train, y_test from 2D to 1D    \n",
    "y_train = y_train.reshape(50000)\n",
    "y_test = y_test.reshape(10000)\n",
    "\n",
    "# Convert class vectors to one hot format\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYg08lysPTOh",
    "outputId": "5ab263ab-5bce-4db9-fb67-cb4c20f4a7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# double check shape\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# expected output:  (50000, 10)\n",
    "# expected output:  (10000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7s9b6KxPV1_"
   },
   "source": [
    "###  Load the pre-trained VGG16 model.  Write your code in the cell below to add each layer in VGG16 (excluding the top layers) to your new model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcBJJ_o_PZxT",
    "outputId": "ccad4f84-46a6-4a26-f632-d2a9b7d5dbcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))   #  first hidden layer\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "for layer in vgg_model.layers:\n",
    "  model.add(layer)\n",
    "\n",
    "# print out the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3eHFwJ4Pd33"
   },
   "source": [
    "### Write your code in the cell below to freeze the weights in each layer in the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zldqPckFPiKs"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72Oy6e0PPjH8"
   },
   "source": [
    "###  Write your code in the cell below to add some \"Dense\" layers as top layers.\n",
    "\n",
    "- Donot forget the output layer\n",
    "- Choose the right activation fucntion for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpi7Zy66PlPL",
    "outputId": "e863fc55-b5d3-49f7-f79f-572319c5dc9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,768,906\n",
      "Trainable params: 1,054,218\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# Add some \"Dense\" layers here, including output layer\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tywy_ZbIPncE"
   },
   "source": [
    "###  Write your code below for compile and fit. \n",
    "\n",
    "### Train your new model. \n",
    "\n",
    "### Notice that you should use earlystopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7uUL_Vy9PsaI",
    "outputId": "1d9328c1-3963-462b-9b42-094d7d778c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 - 38s - loss: 1.2652 - accuracy: 0.5633 - val_loss: 1.0135 - val_accuracy: 0.6479 - 38s/epoch - 96ms/step\n",
      "Epoch 2/10\n",
      "391/391 - 23s - loss: 1.0203 - accuracy: 0.6465 - val_loss: 0.9350 - val_accuracy: 0.6728 - 23s/epoch - 60ms/step\n",
      "Epoch 3/10\n",
      "391/391 - 24s - loss: 0.9495 - accuracy: 0.6653 - val_loss: 0.9040 - val_accuracy: 0.6841 - 24s/epoch - 61ms/step\n",
      "Epoch 4/10\n",
      "391/391 - 24s - loss: 0.9043 - accuracy: 0.6818 - val_loss: 0.9075 - val_accuracy: 0.6865 - 24s/epoch - 61ms/step\n",
      "Epoch 5/10\n",
      "391/391 - 24s - loss: 0.8687 - accuracy: 0.6966 - val_loss: 0.8707 - val_accuracy: 0.6944 - 24s/epoch - 62ms/step\n",
      "Epoch 6/10\n",
      "391/391 - 24s - loss: 0.8385 - accuracy: 0.7053 - val_loss: 0.8760 - val_accuracy: 0.6952 - 24s/epoch - 62ms/step\n",
      "Epoch 7/10\n",
      "391/391 - 24s - loss: 0.8073 - accuracy: 0.7140 - val_loss: 0.8476 - val_accuracy: 0.7025 - 24s/epoch - 63ms/step\n",
      "Epoch 8/10\n",
      "391/391 - 25s - loss: 0.7838 - accuracy: 0.7237 - val_loss: 0.8581 - val_accuracy: 0.7030 - 25s/epoch - 63ms/step\n",
      "Epoch 9/10\n",
      "391/391 - 25s - loss: 0.7670 - accuracy: 0.7300 - val_loss: 0.8488 - val_accuracy: 0.7034 - 25s/epoch - 63ms/step\n",
      "Epoch 10/10\n",
      "391/391 - 25s - loss: 0.7454 - accuracy: 0.7361 - val_loss: 0.8425 - val_accuracy: 0.7059 - 25s/epoch - 64ms/step\n",
      "Elapsed time: 0:04:25.25\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=Adam(learning_rate=0.001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "training = model.fit(new_x_train, y_train,     \n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=2,\n",
    "          validation_data=(new_x_test, y_test))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n",
    "\n",
    "# since we use GPU, the training time for each epoch for the transferred model is about 60 seconds.  \n",
    "# Let it run for a few epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBNzmnpQPvhw"
   },
   "source": [
    "### Write your code below to print out the Precision, Recall, F1 score, and classification_report\n",
    "\n",
    "### Include your findings in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5sH5FDkPwlZ",
    "outputId": "fb54cb58-058e-4765-8ddc-4a980335526f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1s 16ms/step\n",
      "Accuracy Score : 0.7043333333333334\n",
      "Averaged Precision Score : 0.7143708354901369\n",
      "Averaged Recall Score : 0.7043333333333334\n",
      "Averaged F1 Score : 0.7068378640398397\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75       299\n",
      "           1       0.81      0.82      0.82       287\n",
      "           2       0.66      0.62      0.64       318\n",
      "           3       0.49      0.61      0.54       295\n",
      "           4       0.68      0.63      0.65       299\n",
      "           5       0.57      0.58      0.57       290\n",
      "           6       0.70      0.79      0.74       307\n",
      "           7       0.82      0.72      0.77       286\n",
      "           8       0.89      0.75      0.81       316\n",
      "           9       0.82      0.74      0.78       303\n",
      "\n",
      "    accuracy                           0.70      3000\n",
      "   macro avg       0.71      0.70      0.71      3000\n",
      "weighted avg       0.71      0.70      0.71      3000\n",
      "\n",
      "[0.850462019443512, 0.7043333053588867]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = np.argmax(y_test[0:3000], axis=1)\n",
    "prediction = model.predict(new_x_test[0:3000])\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "\n",
    "metrics = { 'Accuracy Score' : metrics.accuracy_score(y_true, prediction),\n",
    "            'Averaged Precision Score' : metrics.precision_score(y_true, prediction, average='weighted'),\n",
    "            'Averaged Recall Score' : metrics.recall_score(y_true, prediction, average='weighted'),\n",
    "            'Averaged F1 Score' : metrics.f1_score(y_true, prediction, average='weighted')}\n",
    "for k,v in metrics.items():\n",
    "  print(f'{k} : {v}')\n",
    "\n",
    "print(classification_report(y_true, prediction))\n",
    "print( model.evaluate(new_x_test[0:3000], y_test[0:3000], verbose=0) )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tulgFkk2Pzs1"
   },
   "source": [
    "### Write your code in the cell below to show 3-5 images in the test set as well as their true labels and their labels predicted by your model with transfer learning. \n",
    "\n",
    "### Include your findings in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "n-N11ttB920h",
    "outputId": "e12330d1-212a-409d-ad08-f81a4ef4439d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photograph #2153... Actual Classification: 8 -> Predicted Classification: 8\n",
      "Photograph #898... Actual Classification: 3 -> Predicted Classification: 3\n",
      "Photograph #1247... Actual Classification: 3 -> Predicted Classification: 3\n",
      "Photograph #1456... Actual Classification: 2 -> Predicted Classification: 2\n",
      "Photograph #1613... Actual Classification: 8 -> Predicted Classification: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "  idx = np.random.randint(1,prediction.shape[0])\n",
    "  plt.imshow(new_x_test[idx])\n",
    "  print(f'Photograph #{idx}... Actual Classification: {y_true[idx]} -> Predicted Classification: {prediction[idx]}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
